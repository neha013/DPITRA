{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-xlaWB_TM4a"
      },
      "source": [
        "#Install, imports and basic code run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsqn6dx5Wi-L"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow==2.10.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iC-_EBcDguO0",
        "outputId": "7e344c2f-7ae6-4f21-b4bb-8ab43df48651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-O1AHepnWa6B"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import imageio\n",
        "import ipywidgets\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pickle\n",
        "# import wandb\n",
        "# from wandb.keras import WandbCallback\n",
        "\n",
        "\n",
        "import PIL\n",
        "from IPython import display\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "import random\n",
        "from tensorflow.keras import layers, Model, Input\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import os\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "# from keras_preprocessing.image import img_to_array, load_img\n",
        "\n",
        "# Setting seed for reproducibility\n",
        "SEED = 42\n",
        "\n",
        "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
        "os.environ['PYTHONHASHSEED']='0'\n",
        "tf.keras.utils.set_random_seed(SEED)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# import tensorflow_addons as tfa\n",
        "from tensorflow.keras.layers import Concatenate, Conv2D, Conv3D, Layer, Dense, Attention, GlobalAveragePooling2D, Lambda,\\\n",
        " Flatten, Reshape, AveragePooling2D, Add, LSTM, Multiply, Softmax, Dropout, LeakyReLU, GRU, TimeDistributed\n",
        "\n",
        "from keras.losses import binary_crossentropy\n",
        "import keras.backend as K\n",
        "from tensorflow.keras.layers import Layer, Input, Concatenate, Conv3D, Add, Dense, Lambda, Activation, Multiply, RepeatVector, Dot\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.activations import sigmoid\n",
        "import tensorflow. keras.backend as K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsaDgjCFxk-c"
      },
      "source": [
        "# PIE/JAAD data and some processing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dtTajyrWpt-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d989faf6-d40e-497b-e49b-8364d8e6ce3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1Vigwxvy5FatoQ2H2rvoSPTzzH7-96luk/obj4\n",
            "---------------------------------------------------------\n",
            "Generating trajectory sequence data\n",
            "fstride: 1\n",
            "sample_type: all\n",
            "height_rng: [0, inf]\n",
            "squarify_ratio: 0\n",
            "data_split_type: random\n",
            "seq_type: crossing\n",
            "min_track_size: 61\n",
            "random_params: {'ratios': None, 'val_data': True, 'regen_data': False}\n",
            "kfold_params: {'num_folds': 5, 'fold': 1}\n",
            "subset: default\n",
            "---------------------------------------------------------\n",
            "Generating database for pie\n",
            "pie annotations loaded from /content/drive/MyDrive/datasets/PIE/data_cache/pie_database.pkl\n",
            "---------------------------------------------------------\n",
            "Generating crossing data\n",
            "Random sample currently exists.\n",
            " Loading from /content/drive/MyDrive/datasets/PIE/data_cache/random_samples.pkl\n",
            "The ratios are [0.5, 0.4, 0.1]\n",
            "Number of train tracks 890\n",
            "Subset: train\n",
            "Number of pedestrians: 890 \n",
            "Total number of samples: 835 \n",
            "---------------------------------------------------------\n",
            "Generating trajectory sequence data\n",
            "fstride: 1\n",
            "sample_type: all\n",
            "height_rng: [0, inf]\n",
            "squarify_ratio: 0\n",
            "data_split_type: random\n",
            "seq_type: crossing\n",
            "min_track_size: 61\n",
            "random_params: {'ratios': None, 'val_data': True, 'regen_data': False}\n",
            "kfold_params: {'num_folds': 5, 'fold': 1}\n",
            "subset: default\n",
            "---------------------------------------------------------\n",
            "Generating database for pie\n",
            "pie annotations loaded from /content/drive/MyDrive/datasets/PIE/data_cache/pie_database.pkl\n",
            "---------------------------------------------------------\n",
            "Generating crossing data\n",
            "Random sample currently exists.\n",
            " Loading from /content/drive/MyDrive/datasets/PIE/data_cache/random_samples.pkl\n",
            "The ratios are [0.5, 0.4, 0.1]\n",
            "Number of val tracks 178\n",
            "Subset: val\n",
            "Number of pedestrians: 178 \n",
            "Total number of samples: 166 \n",
            "---------------------------------------------------------\n",
            "Generating trajectory sequence data\n",
            "fstride: 1\n",
            "sample_type: all\n",
            "height_rng: [0, inf]\n",
            "squarify_ratio: 0\n",
            "data_split_type: random\n",
            "seq_type: crossing\n",
            "min_track_size: 61\n",
            "random_params: {'ratios': None, 'val_data': True, 'regen_data': False}\n",
            "kfold_params: {'num_folds': 5, 'fold': 1}\n",
            "subset: default\n",
            "---------------------------------------------------------\n",
            "Generating database for pie\n",
            "pie annotations loaded from /content/drive/MyDrive/datasets/PIE/data_cache/pie_database.pkl\n",
            "---------------------------------------------------------\n",
            "Generating crossing data\n",
            "Random sample currently exists.\n",
            " Loading from /content/drive/MyDrive/datasets/PIE/data_cache/random_samples.pkl\n",
            "The ratios are [0.5, 0.4, 0.1]\n",
            "Number of test tracks 712\n",
            "Subset: test\n",
            "Number of pedestrians: 712 \n",
            "Total number of samples: 661 \n"
          ]
        }
      ],
      "source": [
        "%cd '/content/drive/MyDrive/obj4'\n",
        "import sys\n",
        "# from sf_gru_o import SFGRU\n",
        "sys.path.insert(0,'/content/drive/MyDrive/datasets/PIE')\n",
        "from pie_data2 import PIE\n",
        "\n",
        "data_opts ={'fstride': 1,\n",
        "            'subset': 'default',\n",
        "            'data_split_type': 'random',  # kfold, random, default\n",
        "            'seq_type': 'crossing',\n",
        "            'min_track_size': 61} ## for obs length of 15 frames + 60 frames tte. This should be adjusted for different setup\n",
        "imdb = PIE(data_path='/content/drive/MyDrive/datasets/PIE') # change with the path to the dataset\n",
        "\n",
        "beh_seq_train = imdb.generate_data_trajectory_sequence('train', **data_opts)\n",
        "beh_seq_val = imdb.generate_data_trajectory_sequence('val', **data_opts)\n",
        "beh_seq_test = imdb.generate_data_trajectory_sequence('test', **data_opts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7LsY3oCAfD3"
      },
      "outputs": [],
      "source": [
        "def convert_normalize_bboxes(all_bboxes, normalize, bbox_type):\n",
        "    '''input box type is x1y1x2y2 in original resolution'''\n",
        "    for i in range(len(all_bboxes)):\n",
        "        if len(all_bboxes[i]) == 0:\n",
        "            continue\n",
        "        bbox = np.array(all_bboxes[i])\n",
        "        # NOTE ltrb to cxcywh\n",
        "        if bbox_type == 'cxcywh':\n",
        "            bbox[..., [2, 3]] = bbox[..., [2, 3]] - bbox[..., [0, 1]]\n",
        "            bbox[..., [0, 1]] += bbox[..., [2, 3]]/2\n",
        "        # NOTE Normalize bbox\n",
        "        if normalize == 'zero-one':\n",
        "            # W, H  = all_resolutions[i][0]\n",
        "            _min = np.array(self.args.min_bbox)[None, :]\n",
        "            _max = np.array(self.args.max_bbox)[None, :]\n",
        "            bbox = (bbox - _min) / (_max - _min)\n",
        "        elif normalize == 'plus-minus-one':\n",
        "            # W, H  = all_resolutions[i][0]\n",
        "            _min = np.array(self.args.min_bbox)[None, :]\n",
        "            _max = np.array(self.args.max_bbox)[None, :]\n",
        "            bbox = (2 * (bbox - _min) / (_max - _min)) - 1\n",
        "        elif normalize == 'none':\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(normalize)\n",
        "        all_bboxes[i] = bbox\n",
        "    return all_bboxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-i5re0aQAl5k"
      },
      "outputs": [],
      "source": [
        "def get_traj_tracks(dataset, data_types, observe_length, predict_length, overlap, normalize):\n",
        "        \"\"\"\n",
        "        Generates tracks by sampling from pedestrian sequences\n",
        "        :param dataset: The raw data passed to the method\n",
        "        :param data_types: Specification of types of data for encoder and decoder. Data types depend on datasets. e.g.\n",
        "        JAAD has 'bbox', 'ceneter' and PIE in addition has 'obd_speed', 'heading_angle', etc.\n",
        "        :param observe_length: The length of the observation (i.e. time steps of the encoder)\n",
        "        :param predict_length: The length of the prediction (i.e. time steps of the decoder)\n",
        "        :param overlap: How much the sampled tracks should overlap. A value between [0,1) should be selected\n",
        "        :param normalize: Whether to normalize center/bounding box coordinates, i.e. convert to velocities. NOTE: when\n",
        "        the tracks are normalized, observation length becomes 1 step shorter, i.e. first step is removed.\n",
        "        :return: A dictinary containing sampled tracks for each data modality\n",
        "        \"\"\"\n",
        "        #  Calculates the overlap in terms of number of frames\n",
        "        seq_length = observe_length + predict_length\n",
        "        overlap_stride = observe_length if overlap == 0 else \\\n",
        "            int((1 - overlap) * observe_length)\n",
        "        overlap_stride = 1 if overlap_stride < 1 else overlap_stride\n",
        "\n",
        "        #  Check the validity of keys selected by user as data type\n",
        "        d = {}\n",
        "        for dt in data_types:\n",
        "            try:\n",
        "                d[dt] = dataset[dt]\n",
        "            except:# KeyError:\n",
        "                raise KeyError('Wrong data type is selected %s' % dt)\n",
        "\n",
        "        d['image'] = dataset['image']\n",
        "        d['pid'] = dataset['pid']\n",
        "        d['age'] = dataset['age']\n",
        "        d['gen'] = dataset['gen']\n",
        "        d['signalized'] = dataset['signalized']\n",
        "        d['Num_lanes'] = dataset['Num_lanes']\n",
        "        d['activities'] = dataset['activities']\n",
        "        # d['resolution'] = dataset['resolution']\n",
        "        # d['flow'] = []\n",
        "        num_trks = len(d['image'])\n",
        "        #  Sample tracks from sequneces\n",
        "        for k in d.keys():\n",
        "            tracks = []\n",
        "            for track in d[k]:\n",
        "                for i in range(0, len(track) - seq_length + 1, overlap_stride):\n",
        "                    tracks.append(track[i:i + seq_length])\n",
        "            d[k] = tracks\n",
        "        #  Normalize tracks using FOL paper method,\n",
        "        d['bbox'] = convert_normalize_bboxes(d['bbox'],'none', 'ltrb')\n",
        "        return d\n",
        "\n",
        "def get_traj_data(data, **model_opts):\n",
        "    \"\"\"\n",
        "    Main data generation function for training/testing\n",
        "    :param data: The raw data\n",
        "    :param model_opts: Control parameters for data generation characteristics (see below for default values)\n",
        "    :return: A dictionary containing training and testing data\n",
        "    \"\"\"\n",
        "\n",
        "    opts = {\n",
        "        'normalize_bbox': True,\n",
        "        'track_overlap': 0.5,\n",
        "        'observe_length': 15,\n",
        "        'predict_length': 45,\n",
        "        'enc_input_type': ['bbox'],\n",
        "        'dec_input_type': [],\n",
        "        'prediction_type': ['bbox']\n",
        "    }\n",
        "    for key, value in model_opts.items():\n",
        "        assert key in opts.keys(), 'wrong data parameter %s' % key\n",
        "        opts[key] = value\n",
        "\n",
        "    observe_length = opts['observe_length']\n",
        "    data_types = set(opts['enc_input_type'] + opts['dec_input_type'] + opts['prediction_type'])\n",
        "    data_tracks = get_traj_tracks(data, data_types, observe_length,\n",
        "                                  opts['predict_length'], opts['track_overlap'],\n",
        "                                  opts['normalize_bbox'])\n",
        "    obs_slices = {}\n",
        "    pred_slices = {}\n",
        "    #  Generate observation/prediction sequences from the tracks\n",
        "    for k in data_tracks.keys():\n",
        "        obs_slices[k] = []\n",
        "        pred_slices[k] = []\n",
        "        # NOTE: Add downsample function\n",
        "        down = 1\n",
        "        obs_slices[k].extend([d[down-1:observe_length:down] for d in data_tracks[k]])\n",
        "        pred_slices[k].extend([d[observe_length+down-1::down] for d in data_tracks[k]])\n",
        "\n",
        "    ret =  {'obs_image': obs_slices['image'],\n",
        "            'obs_pid': obs_slices['pid'],\n",
        "\n",
        "            'pred_image': pred_slices['image'],\n",
        "            'pred_pid': pred_slices['pid'],\n",
        "\n",
        "\n",
        "            'obs_bbox': np.array(obs_slices['bbox']), #enc_input,\n",
        "\n",
        "            'pred_bbox': np.array(pred_slices['bbox']), #pred_target,\n",
        "            'age': np.array(obs_slices['age']),\n",
        "            'gen': np.array(obs_slices['gen']),\n",
        "            'signalized': np.array(obs_slices['signalized']),\n",
        "            'lanes': np.array(obs_slices['Num_lanes']),\n",
        "            'activities': np.array(obs_slices['activities'])\n",
        "            }\n",
        "\n",
        "    return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVPQa9GMAoO0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "traj_model_opts = {'normalize_bbox': True,\n",
        "              'track_overlap': 0.5,\n",
        "              'observe_length': 15,\n",
        "              'predict_length': 45,\n",
        "              'enc_input_type': ['bbox'],\n",
        "              'dec_input_type': [],\n",
        "              'prediction_type': ['bbox']\n",
        "              }\n",
        "\n",
        "traj_model_opts['enc_input_type'].extend(['obd_speed', 'heading_angle'])\n",
        "traj_model_opts['prediction_type'].extend(['obd_speed', 'heading_angle'])\n",
        "\n",
        "train_data = get_traj_data(beh_seq_train, **traj_model_opts)\n",
        "# val_data = get_traj_data(beh_seq_val, **traj_model_opts)\n",
        "# test_data = get_traj_data(beh_seq_test, **traj_model_opts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGHK5l8BeAVq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b685ac2-8285-4dbf-e584-834f631a46c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['obs_image', 'obs_pid', 'pred_image', 'pred_pid', 'obs_bbox', 'pred_bbox', 'age', 'gen', 'signalized', 'lanes', 'activities'])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "train_data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oQ4AeSAGQXK"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def squarify(bbox, squarify_ratio, img_width):\n",
        "    \"\"\"\n",
        "    Changes the ratio of bounding boxes to a fixed ratio\n",
        "    :param bbox: Bounding box tensor [x_min, y_min, x_max, y_max]\n",
        "    :param squarify_ratio: Ratio to be changed to\n",
        "    :param img_width: Image width tensor\n",
        "    :return: Squarified bounding box tensor [x_min, y_min, x_max, y_max]\n",
        "    \"\"\"\n",
        "    width = tf.abs(bbox[0] - bbox[2])\n",
        "    height = tf.abs(bbox[1] - bbox[3])\n",
        "    width_change = height * squarify_ratio - width\n",
        "    bbox_x_min = bbox[0] - width_change / 2\n",
        "    bbox_x_max = bbox[2] + width_change / 2\n",
        "\n",
        "    # Cast img_width to float64\n",
        "    img_width = tf.cast(img_width, tf.float64)\n",
        "\n",
        "    # Clamp bounding box to image borders\n",
        "    bbox_x_min = tf.maximum(bbox_x_min, 0)\n",
        "    bbox_x_max = tf.minimum(bbox_x_max, img_width)\n",
        "\n",
        "    # Concatenate x_min, y_min, x_max, y_max into a tensor\n",
        "    squarified_bbox = tf.stack([bbox_x_min, bbox[1], bbox_x_max, bbox[3]])\n",
        "\n",
        "    return squarified_bbox\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TN6WWDc9GVK9"
      },
      "outputs": [],
      "source": [
        "def bbox_sanity_check(img_size, bbox):\n",
        "\t\"\"\"\n",
        "\tConfirms that the bounding boxes are within image boundaries.\n",
        "\tIf this is not the case, modifications is applied.\n",
        "\t:param img_size: The size of the image\n",
        "\t:param bbox: The bounding box coordinates\n",
        "\t:return: The modified/original bbox\n",
        "\t\"\"\"\n",
        "\timg_width, img_heigth = img_size\n",
        "\tif bbox[0] < 0:\n",
        "\t\tbbox[0] = 0.0\n",
        "\tif bbox[1] < 0:\n",
        "\t\tbbox[1] = 0.0\n",
        "\tif bbox[2] >= img_width:\n",
        "\t\tbbox[2] = img_width - 1\n",
        "\tif bbox[3] >= img_heigth:\n",
        "\t\tbbox[3] = img_heigth - 1\n",
        "\treturn bbox\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6BwJlPtGLLK"
      },
      "outputs": [],
      "source": [
        "\n",
        "def jitter_bbox(img_path, bbox, mode, ratio):\n",
        "\t\"\"\"\n",
        "\tThis method jitters the position or dimensions of the bounding box.\n",
        "\t:param img_path: The to the image\n",
        "\t:param bbox: The bounding box to be jittered\n",
        "\t:param mode: The mode of jittere:\n",
        "\t'same' returns the bounding box unchanged\n",
        "\t\t  'enlarge' increases the size of bounding box based on the given ratio.\n",
        "\t\t  'random_enlarge' increases the size of bounding box by randomly sampling a value in [0,ratio)\n",
        "\t\t  'move' moves the center of the bounding box in each direction based on the given ratio\n",
        "\t\t  'random_move' moves the center of the bounding box in each direction by randomly\n",
        "\t\t\t\t\t\tsampling a value in [-ratio,ratio)\n",
        "\t:param ratio: The ratio of change relative to the size of the bounding box.\n",
        "\t\t   For modes 'enlarge' and 'random_enlarge'\n",
        "\t\t   the absolute value is considered.\n",
        "\t:return: Jittered bounding box\n",
        "\t\"\"\"\n",
        "\n",
        "\tassert(mode in ['same','enlarge','move','random_enlarge','random_move']), \\\n",
        "\t\t\t'mode %s is invalid.' % mode\n",
        "\n",
        "\tif mode == 'same':\n",
        "\t\treturn bbox\n",
        "\n",
        "\timg = load_img(img_path)\n",
        "\n",
        "\tif mode in ['random_enlarge', 'enlarge']:\n",
        "\t\tjitter_ratio  = abs(ratio)\n",
        "\telse:\n",
        "\t\tjitter_ratio  = ratio\n",
        "\n",
        "\tif mode == 'random_enlarge':\n",
        "\t\tjitter_ratio = np.random.random_sample()*jitter_ratio\n",
        "\telif mode == 'random_move':\n",
        "\t\t# for ratio between (-jitter_ratio, jitter_ratio)\n",
        "\t\t# for sampling the formula is [a,b), b > a,\n",
        "\t\t# random_sample * (b-a) + a\n",
        "\t\tjitter_ratio = np.random.random_sample() * jitter_ratio * 2 - jitter_ratio\n",
        "\n",
        "\tjit_boxes = []\n",
        "\tfor b in bbox:\n",
        "\t\tbbox_width = b[2] - b[0]\n",
        "\t\tbbox_height = b[3] - b[1]\n",
        "\n",
        "\t\twidth_change = bbox_width * jitter_ratio\n",
        "\t\theight_change = bbox_height * jitter_ratio\n",
        "\n",
        "\t\tif width_change < height_change:\n",
        "\t\t\theight_change = width_change\n",
        "\t\telse:\n",
        "\t\t\twidth_change = height_change\n",
        "\n",
        "\t\tif mode in ['enlarge','random_enlarge']:\n",
        "\t\t\tb[0] = b[0] - width_change //2\n",
        "\t\t\tb[1] = b[1] - height_change //2\n",
        "\t\telse:\n",
        "\t\t\tb[0] = b[0] + width_change //2\n",
        "\t\t\tb[1] = b[1] + height_change //2\n",
        "\n",
        "\t\tb[2] = b[2] + width_change //2\n",
        "\t\tb[3] = b[3] + height_change //2\n",
        "\n",
        "\t\t# Checks to make sure the bbox is not exiting the image boundaries\n",
        "\t\tb = bbox_sanity_check(img.size, b)\n",
        "\t\tjit_boxes.append(b)\n",
        "\t# elif crop_opts['mode'] == 'border_only':\n",
        "\treturn jit_boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PU9uKm6eJ2PD"
      },
      "outputs": [],
      "source": [
        "def img_pad(img, mode = 'warp', size = 224):\n",
        "\t\"\"\"\n",
        "\tPads a image given the boundries of the box needed\n",
        "\t:param img: The image to be coropped and/or padded\n",
        "\t:param mode: The type of padding or resizing:\n",
        "\t\t\twarp: crops the bounding box and resize to the output size\n",
        "\t\t\tsame: only crops the image\n",
        "\t\t\tpad_same: maintains the original size of the cropped box  and pads with zeros\n",
        "\t\t\tpad_resize: crops the image and resize the cropped box in a way that the longer edge is equal to\n",
        "\t\t\t\t\t\tthe desired output size in that direction while maintaining the aspect ratio. The rest\n",
        "\t\t\t\t\t\tof the image is\tpadded with zeros\n",
        "\t\t\tpad_fit: maintains the original size of the cropped box unless the image is bigger than the size\n",
        "\t\t\t\t\tin which case it scales the image down, and then pads it\n",
        "\t:param size: Target size of image\n",
        "\t:return:\n",
        "\t\"\"\"\n",
        "\tassert(mode in ['same', 'warp', 'pad_same', 'pad_resize', 'pad_fit']), 'Pad mode %s is invalid' % mode\n",
        "\timage = img.copy()\n",
        "\tif mode == 'warp':\n",
        "\t\twarped_image = image.resize((size,size), PIL.Image.NEAREST)\n",
        "\t\treturn warped_image\n",
        "\telif mode == 'same':\n",
        "\t\treturn image\n",
        "\telif mode in ['pad_same', 'pad_resize', 'pad_fit']:\n",
        "\t\timg_size = image.size  # size is in (width, height)\n",
        "\t\tratio = float(size)/max(img_size)\n",
        "\t\tif mode == 'pad_resize' or\t\\\n",
        "\t\t\t(mode == 'pad_fit' and (img_size[0] > size or img_size[1] > size)):\n",
        "\t\t\timg_size = tuple([int(img_size[0]*ratio),int(img_size[1]*ratio)])\n",
        "\t\t\timage = image.resize(img_size, PIL.Image.NEAREST)\n",
        "\t\tpadded_image = PIL.Image.new(\"RGB\", (size, size))\n",
        "\t\tpadded_image.paste(image, ((size-img_size [0])//2,\n",
        "\t\t\t\t\t(size-img_size [1])//2))\n",
        "\t\treturn padded_image\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo1Y30Z4TIYl"
      },
      "source": [
        "#Custom Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oho8ntsRD738"
      },
      "outputs": [],
      "source": [
        "#Custom Data Generator\n",
        "save_path1 = '/content/drive/MyDrive/obj4/features'\n",
        "save_path2 = '/content/drive/MyDrive/obj4/seg_features'\n",
        "# convnet = tf.keras.applications.efficientnet.EfficientNetB4(input_shape=(224, 224, 3),\n",
        "#                               include_top=False, weights='imagenet')\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import PIL\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "class CustomDataGen(tf.keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self,x,y,\n",
        "                 batch_size,\n",
        "                 input_size=(224, 224, 3),\n",
        "                 shuffle=True, train=1):\n",
        "\n",
        "\n",
        "        self.x=x\n",
        "        self.y =y\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.input_size = input_size\n",
        "        self.shuffle = shuffle\n",
        "        self.train= train\n",
        "        self.n = len(self.x[0])\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            ind = np.random.randint(0, len(self.x[0]), len(self.x[0]))\n",
        "            self.x[0] = self.x[0][ind]\n",
        "            self.x[1] = self.x[1][ind]\n",
        "            self.x[2] = self.x[2][ind]\n",
        "\n",
        "\n",
        "            self.y = self.y[ind]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def __get_input(self, path_batch, bbox, target_size, seg=0):\n",
        "\n",
        "\n",
        "      if seg==0:\n",
        "        img_seq=[]\n",
        "        for path,b in zip(path_batch,bbox):\n",
        "\n",
        "           img_seq.append(path)\n",
        "\n",
        "\n",
        "\n",
        "      elif seg==1:\n",
        "\n",
        "        img_seq=[]\n",
        "        for path,b in zip(path_batch,bbox):\n",
        "           seg_path= path.replace(\"images\", \"seg_images\")\n",
        "           img_seq.append(seg_path)\n",
        "\n",
        "      elif seg==4:\n",
        "         img_seq=[]\n",
        "\n",
        "         for c in context:\n",
        "              img_seq.append(c)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      else:\n",
        "          box_seq = []\n",
        "          img_seq=[]\n",
        "            # Calculate minimum and maximum values\n",
        "\n",
        "          x_max = 1920\n",
        "          y_max = 1080\n",
        "\n",
        "\n",
        "            # Normalize the bounding box sequence\n",
        "          for b in bbox:\n",
        "            img_seq.append(b)\n",
        "\n",
        "              # x_bleft, y_bleft, x_tright, y_tright = b\n",
        "              # xbl_normalized = (x_bleft) / (x_max)\n",
        "              # ybl_normalized = (y_bleft) / (y_max)\n",
        "              # xtr_normalized = (x_tright) / (x_max)\n",
        "              # ytr_normalized = (y_tright) / (y_max)\n",
        "\n",
        "\n",
        "\n",
        "              # normalized_b = (\n",
        "              #     xbl_normalized,\n",
        "              #     ybl_normalized,\n",
        "              #     xtr_normalized,\n",
        "              #     ytr_normalized\n",
        "              # )\n",
        "\n",
        "\n",
        "          #     box_seq.append(normalized_b)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      return img_seq\n",
        "\n",
        "\n",
        "    def __get_data(self, batches_b, batches_p, batches_i):\n",
        "        # # Generates data containing batch_size samples\n",
        "\n",
        "\n",
        "        bbox_batch = batches_b\n",
        "        pred_batch = batches_p\n",
        "        image_batch = batches_i\n",
        "        x_im_bat=[]\n",
        "        x_seg_bat=[]\n",
        "        x_bbox_bat=[]\n",
        "        x_pred_bat=[]\n",
        "\n",
        "\n",
        "        for i,j in zip(image_batch, bbox_batch):\n",
        "\n",
        "\n",
        "          x_bbox_bat.append(self.__get_input(i, j, self.input_size, seg=2))\n",
        "        X_bbox_batch =np.asarray(x_bbox_bat)\n",
        "\n",
        "\n",
        "\n",
        "        for i,j in zip(image_batch, pred_batch):\n",
        "\n",
        "\n",
        "          x_pred_bat.append(self.__get_input(i, j, self.input_size, seg=2))\n",
        "        X_pred_batch =np.asarray(x_pred_bat)\n",
        "\n",
        "        for i,j in zip(image_batch, bbox_batch):\n",
        "\n",
        "          inp = self.__get_input(i, j, self.input_size, seg=0)\n",
        "          x_im_bat.append(inp)\n",
        "\n",
        "        X_batch =np.asarray(x_im_bat)\n",
        "\n",
        "        for i,j in zip(image_batch, bbox_batch):\n",
        "          inp = self.__get_input(i, j, self.input_size, seg=1)\n",
        "          x_seg_bat.append(inp)\n",
        "        X_seg_batch =np.asarray(x_seg_bat)\n",
        "\n",
        "\n",
        "\n",
        "        return tf.squeeze(X_bbox_batch, axis=0), tf.squeeze(X_batch, axis=0), tf.squeeze(X_seg_batch, axis=0)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "\n",
        "        batches_obs = self.x[0][index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        batches_pred =  self.x[1][index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        batches_im = self.x[2][index * self.batch_size:(index + 1) * self.batch_size]\n",
        "\n",
        "        batches_y = self.y[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "\n",
        "        X_obs_box, X, X_seg = self.__get_data(batches_obs, batches_pred, batches_im)\n",
        "\n",
        "        # X_obs_box, X_pred_box, X = self.__get_data(batches_obs, batches_pred, batches_im)\n",
        "        return tf.convert_to_tensor(X), tf.convert_to_tensor(X_seg), tf.convert_to_tensor(X_obs_box), np.unique(batches_y)\n",
        "        # return tf.convert_to_tensor(X_obs_box), tf.convert_to_tensor(X_pred_box), tf.convert_to_tensor(X)\n",
        "    def __call__(self):\n",
        "        for i in range(self.__len__()):\n",
        "            yield self.__getitem__(i)\n",
        "\n",
        "            if i == self.__len__()-1:\n",
        "                self.on_epoch_end()\n",
        "    def __len__(self):\n",
        "        return self.n // self.batch_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JDL7d1wIgIl"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE=8\n",
        "x_train = [train_data['obs_bbox'], train_data['pred_bbox'], train_data['obs_image']]\n",
        "y_train = train_data['activities']\n",
        "# x_val = [val_data['obs_bbox'], val_data['pred_bbox'], val_data['obs_image']]\n",
        "# x_test = [test_data['obs_bbox'], test_data['pred_bbox'], test_data['obs_image']]\n",
        "traingen = CustomDataGen(x_train, y_train, batch_size=1)\n",
        "# valgen = CustomDataGen([x_val[0], x_val[1]],batch_size=BATCH_SIZE)\n",
        "# testgen = CustomDataGen([x_test[0], x_test[1]], batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crossing_ind=[]\n",
        "for i in range(len(traingen)):\n",
        "\n",
        "  print(i)\n",
        "  if traingen[i][3]==1:\n",
        "\n",
        "     crossing_ind.append(i)\n"
      ],
      "metadata": {
        "id": "tL5NvhKYuwiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(crossing_ind)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6CYtbqnvNVk",
        "outputId": "ab266de0-45c5-400d-d6da-c90acfa25b7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8379"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "traingen[crossing_ind[0]][3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa6KmTFavUMf",
        "outputId": "71610a26-ac75-4a01-8a62-167c580cf5d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrDHYfE9Zag4"
      },
      "outputs": [],
      "source": [
        "ot = tf.string, tf.string, tf.float64, tf.float64\n",
        "\n",
        "os = (None)\n",
        "ds = tf.data.Dataset.from_generator(traingen,\n",
        "                                    output_types = ot,\n",
        "                                    output_shapes = os)\n",
        "# ds = ds.prefetch(tf.data.AUTOTUNE).cache().batch(8, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTwYmQiXvchl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e0faee8-ac23-4754-d965-d43b68cb1253"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16705208/16705208 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "\n",
        "\n",
        "def preprocess_and_compute_features_img(a,g):\n",
        "    base_model = EfficientNetB0(include_top=False, weights='imagenet')\n",
        "    all_tensors_1 = tf.TensorArray(dtype=tf.float32, size=len(a), dynamic_size=True, clear_after_read=False)\n",
        "\n",
        "\n",
        "    for i in range(len(a)):\n",
        "        flip_condition = tf.strings.regex_full_match(a[i], \".*flip.*\")\n",
        "\n",
        "        image = tf.io.read_file(a[i])  # Read image file\n",
        "        image = tf.image.decode_png(image, channels=3)  # Decode PNG image\n",
        "        image = tf.cond(flip_condition, lambda: tf.image.flip_left_right(image), lambda: image)\n",
        "        try:\n",
        "              box = g[i]\n",
        "              # box = squarify(box, 1, tf.shape(image)[0])\n",
        "              box= tf.cast(box, tf.int32)\n",
        "              offset_height = box[1]\n",
        "              offset_width = box[0]\n",
        "              # target_height = tf.abs(box[3]-box[1])\n",
        "              # target_width = tf.abs(box[2]-box[0])\n",
        "              target_height = tf.maximum(tf.abs(box[3] - box[1]), 1)\n",
        "              target_width = tf.maximum(tf.abs(box[2] - box[0]), 1)\n",
        "\n",
        "              # Check if target_width is less than or equal to 0\n",
        "\n",
        "              cropped_image = tf.image.crop_to_bounding_box(image, offset_height, offset_width, target_height, target_width)\n",
        "        except Exception as e:\n",
        "              cropped_image = image\n",
        "        # # Resize or pad the cropped image to 224x224\n",
        "        cropped_image = tf.image.resize_with_pad(cropped_image, 224, 224)\n",
        "\n",
        "        # Preprocess image for EfficientNet\n",
        "        image = tf.cast(cropped_image, tf.float32)  # Cast image to float32\n",
        "        image /= 255.0\n",
        "        image = tf.keras.applications.efficientnet.preprocess_input(image)  # Preprocess image for EfficientNet\n",
        "        features = base_model(tf.expand_dims(image, 0))  # Compute EfficientNet features\n",
        "        features = tf.squeeze(GlobalAveragePooling2D()(features))\n",
        "        all_tensors_1=all_tensors_1.write(i, features)\n",
        "    stacked_tensor_1 = all_tensors_1.stack()\n",
        "    return stacked_tensor_1\n",
        "\n",
        "def preprocess_and_compute_features_seg(a):\n",
        "    # a,b,c,d,e,f,g =x\n",
        "\n",
        "    base_model = EfficientNetB0(include_top=False, weights='imagenet')\n",
        "    all_tensors_2 = tf.TensorArray(dtype=tf.float32, size=len(a), dynamic_size=True, clear_after_read=False)\n",
        "\n",
        "\n",
        "    for i in range(len(a)):\n",
        "        flip_condition = tf.strings.regex_full_match(a[i], \".*flip.*\")\n",
        "\n",
        "        image = tf.io.read_file(a[i])  # Read image file\n",
        "        image = tf.image.decode_png(image, channels=3)  # Decode PNG image\n",
        "        image = tf.cond(flip_condition, lambda: tf.image.flip_left_right(image), lambda: image)\n",
        "        image = tf.image.resize(image, (224, 224))  # Resize image to fit EfficientNet input size\n",
        "        image = tf.cast(image, tf.float32)  # Cast image to float32\n",
        "        image = tf.keras.applications.efficientnet.preprocess_input(image)  # Preprocess image for EfficientNet\n",
        "        features = base_model(tf.expand_dims(image, 0))  # Compute EfficientNet features\n",
        "        features = tf.squeeze(GlobalAveragePooling2D()(features))\n",
        "        all_tensors_2=all_tensors_2.write(i, features)\n",
        "    stacked_tensor_2 = all_tensors_2.stack()\n",
        "    return stacked_tensor_2\n",
        "\n",
        "def preprocess_bbox(g):\n",
        "    img_height=1080\n",
        "    img_width=1920\n",
        "\n",
        "    all_tensors_3 = tf.TensorArray(dtype=tf.float64, size=len(g), dynamic_size=True, clear_after_read=False)\n",
        "\n",
        "    for k in range(len(g)):\n",
        "        bbox = g[k]\n",
        "        x_1, y_1, x_2, y_2 = tf.split(bbox, 4, axis=-1)\n",
        "\n",
        "        # Normalize bounding box coordinates\n",
        "        y_1 = y_1 / img_height\n",
        "        x_1 = x_1 / img_width\n",
        "        y_2 = y_2 / img_height\n",
        "        x_2 = x_2 / img_width\n",
        "\n",
        "        normalized_bbox = tf.concat([x_1, y_1, x_2, y_2], axis=-1)\n",
        "        all_tensors_3 = all_tensors_3.write(k, normalized_bbox)\n",
        "\n",
        "    stacked_tensor_3 = all_tensors_3.stack()\n",
        "    return stacked_tensor_3\n",
        "\n",
        "def encode(data, categories):\n",
        "  one_hot_encoded = tf.squeeze(tf.one_hot(tf.cast(data, tf.int32), depth=categories))\n",
        "  return one_hot_encoded\n",
        "\n",
        "\n",
        "dataset=ds\n",
        "# Map preprocessing and feature computation function to the dataset for the first and fourth elements\n",
        "dataset = dataset.map(lambda a,d,g,y: (preprocess_and_compute_features_img(a,g),preprocess_and_compute_features_seg(d),g, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "dataset = dataset.map(lambda a,d,g,y: (a,d,preprocess_bbox(g),y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "dataset = dataset.filter(lambda a,d,g,y: tf.shape(a[0])[0] > 0)\n",
        "\n",
        "def prepare_dataset(a, d, g, y):\n",
        "    x = (a, d, g)  # Combine inputs into a tuple\n",
        "    threshold = 0.5\n",
        "    binary_labels = tf.cast(y >= threshold, tf.int32)\n",
        "    return x, tf.squeeze(binary_labels,axis=-1)\n",
        "\n",
        "# Map the dataset to prepare the data\n",
        "dataset = dataset.map(prepare_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "# Prefetch the dataset\n",
        "dataset = dataset.prefetch(tf.data.AUTOTUNE).batch(8, num_parallel_calls=tf.data.AUTOTUNE).cache()\n",
        "# dataset = dataset.prefetch(tf.data.AUTOTUNE).cache()\n",
        "# Filter out examples with empty images (assuming image is the first element in the dataset tuple)\n",
        "\n",
        "\n",
        "# Example usage of the dataset\n",
        "# for x,y in dataset.take(1):\n",
        "#     a,b,c,d,e,f,g=x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for x,y in dataset.take(1):\n",
        "#     a,b,c=x\n",
        "# print(c.shape)"
      ],
      "metadata": {
        "id": "RQJc2MrsvdID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_vG-NiMxvR0"
      },
      "source": [
        "# Model architecture modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEIaYyQQNMYM"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "class TubeletEmbedding(layers.Layer):\n",
        "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim =embed_dim\n",
        "        self.patch_size = patch_size\n",
        "        self.projection =tf.keras.layers.Conv1D(\n",
        "    self.embed_dim,\n",
        "    3,\n",
        "    strides=1,\n",
        "    padding='same',\n",
        "    data_format=None,\n",
        "    dilation_rate=1,\n",
        "    groups=1,\n",
        "    activation=None,\n",
        "    use_bias=True,\n",
        "    kernel_initializer='glorot_uniform',\n",
        "    bias_initializer='zeros',\n",
        "    kernel_regularizer=None,\n",
        "    bias_regularizer=None,\n",
        "    activity_regularizer=None,\n",
        "    kernel_constraint=None,\n",
        "    bias_constraint=None,\n",
        "    **kwargs\n",
        ")\n",
        "        self.projection2 = layers.GRU(self.embed_dim, return_sequences=True, return_state=True)\n",
        "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
        "\n",
        "    def call(self, videos, vid):\n",
        "        if vid==0:\n",
        "          projected_patches = self.projection(videos)\n",
        "\n",
        "        elif vid==1:\n",
        "          projected_patches,_ = self.projection2(videos)\n",
        "\n",
        "\n",
        "        flattened_patches = self.flatten(projected_patches)\n",
        "        return flattened_patches\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"patch_size\": self.patch_size,\n",
        "        })\n",
        "        return config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dh2aN2Q1Nwur"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PositionalEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        _, num_tokens, _ = input_shape\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_tokens, output_dim=self.embed_dim\n",
        "        )\n",
        "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
        "\n",
        "    def call(self, encoded_tokens):\n",
        "        # Encode the positions and add it to the encoded tokens\n",
        "        encoded_positions = self.position_embedding(self.positions)\n",
        "        encoded_tokens = encoded_tokens + encoded_positions\n",
        "        return encoded_tokens\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim\n",
        "\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YK6KO7ZoNyW5"
      },
      "outputs": [],
      "source": [
        "# DATA\n",
        "BATCH_SIZE = 2\n",
        "INPUT_SHAPE = (15, 1796)\n",
        "INPUT_SHAPE2 = (15, 4)\n",
        "INPUT_SHAPE4 =(15,)\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "# OPTIMIZER\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 1e-5\n",
        "\n",
        "# TRAINING\n",
        "EPOCHS = 50\n",
        "\n",
        "# TUBELET EMBEDDING\n",
        "PATCH_SIZE = (2,8,8)\n",
        "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
        "\n",
        "# ViViT ARCHITECTURE\n",
        "LAYER_NORM_EPS = 1e-6\n",
        "PROJECTION_DIM = 128\n",
        "# PROJECTION_DIM2= 4\n",
        "NUM_HEADS = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxnqCeIyaCJT"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class DenoisingUNet(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, **kwargs):\n",
        "        super(DenoisingUNet, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.encoder1 = self.conv_block(embed_dim // 4)\n",
        "        self.encoder2 = self.conv_block(embed_dim // 2)\n",
        "        self.middle = self.conv_block(embed_dim)\n",
        "        self.decoder2 = self.deconv_block(embed_dim // 2)\n",
        "        self.decoder1 = self.deconv_block(embed_dim // 4)\n",
        "        self.final_conv = layers.Conv1D(embed_dim, kernel_size=1, padding='same')\n",
        "\n",
        "    def conv_block(self, filters):\n",
        "        block = tf.keras.Sequential([\n",
        "            layers.Conv1D(filters, kernel_size=3, padding='same', activation='relu'),\n",
        "            layers.Conv1D(filters, kernel_size=3, padding='same', activation='relu')\n",
        "        ])\n",
        "        return block\n",
        "\n",
        "    def deconv_block(self, filters):\n",
        "        block = tf.keras.Sequential([\n",
        "            layers.Conv1DTranspose(filters, kernel_size=3, strides=2, padding='same', activation='relu'),\n",
        "            layers.Conv1D(filters, kernel_size=3, padding='same', activation='relu')\n",
        "        ])\n",
        "        return block\n",
        "\n",
        "    def pad_or_crop(self, tensor, target_shape):\n",
        "        current_shape = tf.shape(tensor)[1]\n",
        "        diff = target_shape - current_shape\n",
        "        pad_left = tf.maximum(diff // 2, 0)\n",
        "        pad_right = tf.maximum(diff - pad_left, 0)\n",
        "        crop_left = tf.maximum(-diff // 2, 0)\n",
        "        crop_right = tf.maximum(-diff - crop_left, 0)\n",
        "        tensor = tensor[:, crop_left:current_shape-crop_right, :]\n",
        "        tensor = tf.pad(tensor, [[0, 0], [pad_left, pad_right], [0, 0]], mode='CONSTANT')\n",
        "        return tensor\n",
        "\n",
        "    def call(self, x):\n",
        "        e1 = self.encoder1(x)\n",
        "        e2 = self.encoder2(layers.MaxPooling1D(pool_size=2)(e1))\n",
        "        m = self.middle(layers.MaxPooling1D(pool_size=2)(e2))\n",
        "        d2 = self.decoder2(layers.UpSampling1D(size=2)(m))\n",
        "        d2 = self.pad_or_crop(d2, tf.shape(e2)[1])\n",
        "        d2 = layers.Concatenate()([d2, e2])\n",
        "        d1 = self.decoder1(layers.UpSampling1D(size=2)(d2))\n",
        "        d1 = self.pad_or_crop(d1, tf.shape(e1)[1])\n",
        "        d1 = layers.Concatenate()([d1, e1])\n",
        "        return self.final_conv(d1)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim\n",
        "        })\n",
        "        return config\n",
        "class PDAM(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim=128, max_iters=5, tolerance=1e-3, noise_std=0.1, **kwargs):\n",
        "        super(PDAM, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_iters = max_iters\n",
        "        self.tolerance = tolerance\n",
        "        self.noise_std = noise_std\n",
        "        self.query = layers.Dense(units=embed_dim)\n",
        "        self.key = layers.Dense(units=embed_dim)\n",
        "        self.value = layers.Dense(units=embed_dim)\n",
        "        self.denoise_net = DenoisingUNet(embed_dim)\n",
        "\n",
        "    def add_noise(self, tensor):\n",
        "        noise = tf.random.normal(shape=tf.shape(tensor), mean=0.0, stddev=self.noise_std, dtype=tf.float32)\n",
        "        return tensor + noise\n",
        "\n",
        "    def self_attention(self, Q, K, V):\n",
        "        d_k = tf.cast(self.embed_dim, dtype=tf.float32)\n",
        "        score = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(d_k)\n",
        "        score = tf.nn.softmax(score, axis=-1)\n",
        "        Z = tf.matmul(score, V)\n",
        "        return Z\n",
        "\n",
        "    def call(self, x1, x2):\n",
        "        Q = self.query(x1)\n",
        "        K = self.key(x1)\n",
        "        V = self.value(x2)\n",
        "        Q = self.add_noise(Q)\n",
        "        K = self.add_noise(K)\n",
        "        V = self.add_noise(V)\n",
        "        Z = self.self_attention(Q, K, V)\n",
        "        prev_Z = tf.zeros_like(Z)\n",
        "\n",
        "        def loop_cond(i, Z, prev_Z, Q, K, V):\n",
        "            return tf.logical_and(tf.less(i, self.max_iters), tf.greater(tf.reduce_mean(tf.abs(Z - prev_Z)), self.tolerance))\n",
        "\n",
        "        def loop_body(i, Z, prev_Z, Q, K, V):\n",
        "            prev_Z = Z\n",
        "            Z = self.self_attention(Q, K, V)\n",
        "            Z = tf.nn.softmax(self.denoise_net(Z), axis=-1)\n",
        "            Q, K, V = Z, Z, Z\n",
        "            Q = self.add_noise(Q)\n",
        "            K = self.add_noise(K)\n",
        "            V = self.add_noise(V)\n",
        "            return tf.add(i, 1), Z, prev_Z, Q, K, V\n",
        "\n",
        "        i = tf.constant(0)\n",
        "        i, Z, prev_Z, Q, K, V = tf.while_loop(loop_cond, loop_body, [i, Z, prev_Z, Q, K, V], shape_invariants=[i.get_shape(), tf.TensorShape([None, None, self.embed_dim]), tf.TensorShape([None, None, self.embed_dim]), Q.get_shape(), K.get_shape(), V.get_shape()])\n",
        "        return Z\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"max_iters\": self.max_iters,\n",
        "            \"tolerance\": self.tolerance,\n",
        "            \"noise_std\": self.noise_std\n",
        "        })\n",
        "        return config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZaUo76ON2A7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95d1912c-1246-44cb-80b9-faf7e0584b02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 15, 1280)]   0           []                               \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 15, 4)]      0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 15, 1280)]   0           []                               \n",
            "                                                                                                  \n",
            " tubelet_embedding (TubeletEmbe  (None, 15, 128)     543104      ['input_1[0][0]',                \n",
            " dding)                                                           'input_3[0][0]',                \n",
            "                                                                  'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " positional_encoder (Positional  (None, 15, 128)     1920        ['tubelet_embedding[0][0]',      \n",
            " Encoder)                                                         'tubelet_embedding[2][0]']      \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 15, 128)     256         ['positional_encoder[0][0]']     \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " layer_normalization_1 (LayerNo  (None, 15, 128)     256         ['tubelet_embedding[1][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " layer_normalization_4 (LayerNo  (None, 15, 128)     256         ['positional_encoder[1][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " layer_normalization_5 (LayerNo  (None, 15, 128)     256         ['tubelet_embedding[1][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " pdam (PDAM)                    (None, None, 128)    218240      ['layer_normalization[0][0]',    \n",
            "                                                                  'layer_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " pdam_1 (PDAM)                  (None, None, 128)    218240      ['layer_normalization_4[0][0]',  \n",
            "                                                                  'layer_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 15, 128)      0           ['pdam[0][0]',                   \n",
            "                                                                  'positional_encoder[0][0]']     \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 15, 128)      0           ['pdam_1[0][0]',                 \n",
            "                                                                  'positional_encoder[1][0]']     \n",
            "                                                                                                  \n",
            " sequential (Sequential)        (None, 15, 128)      131712      ['add[0][0]',                    \n",
            "                                                                  'add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " encoded_R (Add)                (None, 15, 128)      0           ['sequential[0][0]',             \n",
            "                                                                  'add[0][0]']                    \n",
            "                                                                                                  \n",
            " encoded_S (Add)                (None, 15, 128)      0           ['sequential[1][0]',             \n",
            "                                                                  'add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_3 (LayerNo  (None, 15, 128)     256         ['encoded_R[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " layer_normalization_7 (LayerNo  (None, 15, 128)     256         ['encoded_S[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " global_average_pooling1d_1 (Gl  (None, 128)         0           ['layer_normalization_3[0][0]']  \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " global_average_pooling1d_2 (Gl  (None, 128)         0           ['layer_normalization_7[0][0]']  \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 128)          0           ['global_average_pooling1d_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 128)          0           ['global_average_pooling1d_2[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " tf.math.multiply (TFOpLambda)  (None, 128)          0           ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.multiply_1 (TFOpLambda  (None, 128)         0           ['dropout_2[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 128)         0           ['tubelet_embedding[1][0]']      \n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 128)         0           ['tf.math.multiply[0][0]',       \n",
            " da)                                                              'tf.math.multiply_1[0][0]']     \n",
            "                                                                                                  \n",
            " tf.math.multiply_2 (TFOpLambda  (None, 128)         0           ['global_average_pooling1d[0][0]'\n",
            " )                                                               ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TFOpLa  (None, 128)         0           ['tf.__operators__.add[0][0]',   \n",
            " mbda)                                                            'tf.math.multiply_2[0][0]']     \n",
            "                                                                                                  \n",
            " final (Dense)                  (None, 1)            129         ['tf.__operators__.add_1[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,114,881\n",
            "Trainable params: 1,114,881\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "tubelet_embedder=TubeletEmbedding(embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE)\n",
        "positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM)\n",
        "input_shape=INPUT_SHAPE\n",
        "num_heads=NUM_HEADS\n",
        "embed_dim=PROJECTION_DIM\n",
        "layer_norm_eps=LAYER_NORM_EPS\n",
        "num_classes=NUM_CLASSES\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "\n",
        "# Hyperparameters (adjust as needed)\n",
        "attention_dim = 256\n",
        "timesteps =15\n",
        "\n",
        "# Pedestrian Attributes Branch\n",
        "rgb_input = layers.Input(shape=(timesteps, 1280))\n",
        "ped_attr = rgb_input\n",
        "\n",
        "\n",
        "# Scene Attributes Branch\n",
        "seg_input = layers.Input(shape=(timesteps, 1280))\n",
        "scene_attr = seg_input\n",
        "\n",
        "traj_input = layers.Input(shape=(timesteps, 4))\n",
        "\n",
        "Feed_MLP = keras.Sequential(\n",
        "      [\n",
        "          layers.Dense(units=embed_dim * 4, activation=tf.nn.gelu, kernel_initializer=keras.initializers.HeNormal(),kernel_regularizer= keras.regularizers.L2(1e-6) ),\n",
        "          layers.Dropout(0.5),\n",
        "          layers.Dense(units=embed_dim, activation=tf.nn.gelu, kernel_initializer=keras.initializers.HeNormal(),kernel_regularizer= keras.regularizers.L2(1e-6) ),\n",
        "\n",
        "      ])\n",
        "\n",
        "\n",
        "patches_0 = tubelet_embedder(ped_attr, 0)\n",
        "# Encode patches.\n",
        "encoded_patches_0 = positional_encoder(patches_0)\n",
        "x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches_0)\n",
        "\n",
        "#Get the second input\n",
        "encoded_patches_1 = tubelet_embedder(traj_input, 1)\n",
        "traj_encoded = layers.GlobalAvgPool1D()(encoded_patches_1)\n",
        "# Encode patches.\n",
        "x2 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches_1)\n",
        "\n",
        "\n",
        "attention_output = PDAM()(x1,x2)\n",
        "\n",
        "x3 = layers.Add()([attention_output, encoded_patches_0])\n",
        "\n",
        "x4 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "x4 = Feed_MLP(x3)\n",
        "\n",
        "# Skip connection\n",
        "encoded_patches = layers.Add(name='encoded_R')([x4, x3])\n",
        "representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
        "representation = layers.GlobalAvgPool1D()(representation)\n",
        "representation =layers.Dropout(0.5)(representation)\n",
        "\n",
        "\n",
        "# Create patches.\n",
        "patches_0 = tubelet_embedder(scene_attr, 0)\n",
        "# Encode patches.\n",
        "encoded_patches_0 = positional_encoder(patches_0)\n",
        "x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches_0)\n",
        "\n",
        "x2 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches_1)\n",
        "\n",
        "attention_output = PDAM()(x1,x2)\n",
        "\n",
        "x3 = layers.Add()([attention_output, encoded_patches_0])\n",
        "\n",
        "x4 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "x4 = Feed_MLP(x3)\n",
        "\n",
        "# Skip connection\n",
        "encoded_patches = layers.Add(name='encoded_S')([x4, x3])\n",
        "representation2 = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
        "representation2 = layers.GlobalAvgPool1D()(representation2)\n",
        "representation2 =layers.Dropout(0.5)(representation2)\n",
        "\n",
        "# Define trainable weights for adaptive addition\n",
        "weight_representation = tf.Variable(1.0, trainable=True, name='weight_representation')\n",
        "weight_representation2 = tf.Variable(1.0, trainable=True, name='weight_representation2')\n",
        "weight_traj_encoded = tf.Variable(1.0, trainable=True, name='weight_traj_encoded')\n",
        "\n",
        "# Stack weights and apply softmax to get normalized weights\n",
        "weights = tf.stack([weight_representation, weight_representation2, weight_traj_encoded])\n",
        "normalized_weights = tf.nn.softmax(weights)\n",
        "\n",
        "# Compute weighted sum of input tensors\n",
        "final_rep = (normalized_weights[0] * representation +\n",
        "             normalized_weights[1] * representation2 +\n",
        "             normalized_weights[2] * traj_encoded)\n",
        "\n",
        "# Classify outputs.\n",
        "output_r = layers.Dense(units=1, activation='sigmoid', name='final', kernel_initializer=keras.initializers.HeNormal())(final_rep)\n",
        "\n",
        "model = keras.Model(inputs=[rgb_input,seg_input,traj_input], outputs=[output_r])\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azawQVU2x1Qo"
      },
      "source": [
        "# Model Training- Phase-I"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kgmSZxuOE-t"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=tf.keras.metrics.BinaryAccuracy())\n",
        "steps_per_epoch = len(traingen)/8\n",
        "# steps_per_epoch =60\n",
        "model.fit(dataset,epochs=1, steps_per_epoch=steps_per_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Training- Phase-II"
      ],
      "metadata": {
        "id": "v_az--Shm6Wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the existing inputs from the model\n",
        "inputs = model.input\n",
        "\n",
        "# Define the counterfactual RGB input (all zeros)\n",
        "# rgb_input_counterfactual = layers.Input(shape=(timesteps, 1280), name='rgb_input_counterfactual')\n",
        "rgb_input_counterfactual = tf.zeros_like(rgb_input)\n",
        "# Replace the original RGB input with the counterfactual one for the new model\n",
        "counterfactual_inputs = [rgb_input_counterfactual if inp.name == 'rgb_input' else inp for inp in inputs]\n",
        "\n",
        "# Get the outputs from the original model\n",
        "original_output = model.output\n",
        "\n",
        "# Create a new model with the counterfactual input\n",
        "counterfactual_model = keras.Model(inputs=counterfactual_inputs, outputs=original_output)\n",
        "\n",
        "# Compile the model with a custom loss function\n",
        "counterfactual_model.compile(optimizer='adam', loss='binary_crossentropy',metrics=tf.keras.metrics.BinaryAccuracy())\n"
      ],
      "metadata": {
        "id": "DergdSl0ph2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "\n",
        "# Define the loss function\n",
        "def counterfactual_loss(y_true, y_pred_normal, y_pred_counterfactual):\n",
        "    prediction_loss = binary_crossentropy(y_true, y_pred_normal)\n",
        "    counterfactual_diff_loss = tf.reduce_mean(tf.square(y_pred_normal - y_pred_counterfactual))\n",
        "    return prediction_loss + counterfactual_diff_loss\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "counterfactual_model.fit(dataset,epochs=1, steps_per_epoch=steps_per_epoch)"
      ],
      "metadata": {
        "id": "kPk_Nz47pmzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Training- Phase-III"
      ],
      "metadata": {
        "id": "AXk7RsW5qjZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the existing inputs from the model\n",
        "inputs = counterfactual_model.input\n",
        "\n",
        "# Define the counterfactual RGB input (all zeros)\n",
        "seg_input_counterfactual = tf.zeros_like(seg_input)\n",
        "# Replace the original RGB input with the counterfactual one for the new model\n",
        "counterfactual_inputs = [seg_input_counterfactual if inp.name == 'seg_input' else inp for inp in inputs]\n",
        "counterfactual_inputs = [rgb_input if inp.name == 'counterfactual_rgb_input' else inp for inp in inputs]\n",
        "# Get the outputs from the original model\n",
        "original_output = counterfactual_model.output\n",
        "\n",
        "# Create a new model with the counterfactual input\n",
        "counterfactual_model2 = keras.Model(inputs=counterfactual_inputs, outputs=original_output)\n",
        "\n",
        "# Compile the model with a custom loss function\n",
        "counterfactual_model2.compile(optimizer='adam', loss='binary_crossentropy',metrics=tf.keras.metrics.BinaryAccuracy())\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "counterfactual_model2.fit(dataset,epochs=1, steps_per_epoch=steps_per_epoch)"
      ],
      "metadata": {
        "id": "ykNGg4Fkqd66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Representation model"
      ],
      "metadata": {
        "id": "oD5dK8QmNvoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the existing inputs from the model\n",
        "inputs = counterfactual_model2.input\n",
        "\n",
        "counterfactual_inputs = [seg_input if inp.name == 'counterfactual_seg_input' else inp for inp in inputs]\n",
        "counterfactual_inputs = [rgb_input if inp.name == 'counterfactual_rgb_input' else inp for inp in inputs]\n",
        "\n",
        "Rep_model = keras.Model(inputs=inputs, outputs=[counterfactual_model2.get_layer('encoded_R').output, counterfactual_model2.get_layer('encoded_S').output])\n",
        "\n",
        "# Verify the new model's architecture\n",
        "Rep_model.summary()\n"
      ],
      "metadata": {
        "id": "0CdO1y05rdg4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca481e70-2ae2-4daa-aac2-f4debeb5256d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 15, 1280)]   0           []                               \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 15, 4)]      0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 15, 1280)]   0           []                               \n",
            "                                                                                                  \n",
            " tubelet_embedding (TubeletEmbe  (None, 15, 128)     543104      ['input_1[0][0]',                \n",
            " dding)                                                           'input_3[0][0]',                \n",
            "                                                                  'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " positional_encoder (Positional  (None, 15, 128)     1920        ['tubelet_embedding[0][0]',      \n",
            " Encoder)                                                         'tubelet_embedding[2][0]']      \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 15, 128)     256         ['positional_encoder[0][0]']     \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " layer_normalization_1 (LayerNo  (None, 15, 128)     256         ['tubelet_embedding[1][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " layer_normalization_4 (LayerNo  (None, 15, 128)     256         ['positional_encoder[1][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " layer_normalization_5 (LayerNo  (None, 15, 128)     256         ['tubelet_embedding[1][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " pdam (PDAM)                    (None, None, 128)    218240      ['layer_normalization[0][0]',    \n",
            "                                                                  'layer_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " pdam_1 (PDAM)                  (None, None, 128)    218240      ['layer_normalization_4[0][0]',  \n",
            "                                                                  'layer_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 15, 128)      0           ['pdam[0][0]',                   \n",
            "                                                                  'positional_encoder[0][0]']     \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 15, 128)      0           ['pdam_1[0][0]',                 \n",
            "                                                                  'positional_encoder[1][0]']     \n",
            "                                                                                                  \n",
            " sequential (Sequential)        (None, 15, 128)      131712      ['add[0][0]',                    \n",
            "                                                                  'add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " encoded_R (Add)                (None, 15, 128)      0           ['sequential[0][0]',             \n",
            "                                                                  'add[0][0]']                    \n",
            "                                                                                                  \n",
            " encoded_S (Add)                (None, 15, 128)      0           ['sequential[1][0]',             \n",
            "                                                                  'add_1[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,114,240\n",
            "Trainable params: 1,114,240\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the model"
      ],
      "metadata": {
        "id": "l7x-MukraI7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the entire model\n",
        "Rep_model.save('/content/drive/MyDrive/obj-5/intent_model.tf')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJ_rU7nJZ12o",
        "outputId": "e3dd56cd-8316-48eb-b1de-73448946bd45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as conv1d_layer_call_fn, conv1d_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, reshape_layer_call_fn, reshape_layer_call_and_return_conditional_losses while saving (showing 5 of 51). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "intent_model = load_model('/content/drive/MyDrive/obj-5/intent_model.h5', custom_objects={\n",
        "    'TubeletEmbedding': TubeletEmbedding,\n",
        "    'PositionalEncoder': PositionalEncoder,\n",
        "    'PDAM': PDAM,\n",
        "    'DenoisingUNet': DenoisingUNet\n",
        "})"
      ],
      "metadata": {
        "id": "AzmN2bhQaV90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2f776b7-3642-4567-cce0-fe8bbce16b00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RSol9Qw-EBjv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "q-xlaWB_TM4a",
        "FsaDgjCFxk-c"
      ],
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}