{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9laZQWlrmg-"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUJCad70ybuf"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow==2.10.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGiISi3nrmg_"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import imageio\n",
        "import ipywidgets\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pickle\n",
        "# import wandb\n",
        "# from wandb.keras import WandbCallback\n",
        "\n",
        "\n",
        "import PIL\n",
        "from IPython import display\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "import random\n",
        "# from keras.utils import to_categorical\n",
        "from tensorflow.keras import layers, Model, Input\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import os\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "# from keras_preprocessing.image import img_to_array, load_img\n",
        "\n",
        "# Setting seed for reproducibility\n",
        "SEED = 42\n",
        "\n",
        "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
        "os.environ['PYTHONHASHSEED']='0'\n",
        "tf.keras.utils.set_random_seed(SEED)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# import tensorflow_addons as tfa\n",
        "from tensorflow.keras.layers import Concatenate, Conv2D, Conv3D, Layer, Dense, Attention, GlobalAveragePooling2D, Lambda,\\\n",
        " Flatten, Reshape, AveragePooling2D, Add, LSTM, Multiply, Softmax, Dropout, LeakyReLU, GRU, TimeDistributed, BatchNormalization\n",
        "\n",
        "from keras.losses import binary_crossentropy\n",
        "import keras.backend as K\n",
        "from tensorflow.keras.layers import Layer, Input, Concatenate, Conv3D, Add, Dense, Lambda, Activation, Multiply, RepeatVector\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.activations import sigmoid\n",
        "import tensorflow. keras.backend as K\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, AveragePooling2D, Flatten, Attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpgFLutHzHvr",
        "outputId": "01ba0d14-ab72-4e38-e1b8-b9639beb0771"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.10.1\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVKCciuXuE_s",
        "outputId": "06e3bd7f-9910-48dd-911c-5b085d0b7f34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bATUJdV9rmhC"
      },
      "source": [
        "## Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM4dgkr0PSLS",
        "outputId": "765f2ee5-1a12-4600-b71c-7d3010a85dc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/17b-e8t1hy73tyXHfAt4dwe7vTH5C_SA2/obj-3\n",
            "---------------------------------------------------------\n",
            "Generating trajectory sequence data\n",
            "fstride: 1\n",
            "sample_type: all\n",
            "height_rng: [0, inf]\n",
            "squarify_ratio: 0\n",
            "data_split_type: random\n",
            "seq_type: crossing\n",
            "min_track_size: 61\n",
            "random_params: {'ratios': None, 'val_data': True, 'regen_data': False}\n",
            "kfold_params: {'num_folds': 5, 'fold': 1}\n",
            "subset: default\n",
            "---------------------------------------------------------\n",
            "Generating database for pie\n",
            "pie annotations loaded from /content/drive/MyDrive/datasets/PIE/data_cache/pie_database.pkl\n",
            "---------------------------------------------------------\n",
            "Generating crossing data\n",
            "Random sample currently exists.\n",
            " Loading from /content/drive/MyDrive/datasets/PIE/data_cache/random_samples.pkl\n",
            "The ratios are [0.5, 0.4, 0.1]\n",
            "Number of train tracks 890\n",
            "Subset: train\n",
            "Number of pedestrians: 890 \n",
            "Total number of samples: 835 \n",
            "---------------------------------------------------------\n",
            "Generating trajectory sequence data\n",
            "fstride: 1\n",
            "sample_type: all\n",
            "height_rng: [0, inf]\n",
            "squarify_ratio: 0\n",
            "data_split_type: random\n",
            "seq_type: crossing\n",
            "min_track_size: 61\n",
            "random_params: {'ratios': None, 'val_data': True, 'regen_data': False}\n",
            "kfold_params: {'num_folds': 5, 'fold': 1}\n",
            "subset: default\n",
            "---------------------------------------------------------\n",
            "Generating database for pie\n",
            "pie annotations loaded from /content/drive/MyDrive/datasets/PIE/data_cache/pie_database.pkl\n",
            "---------------------------------------------------------\n",
            "Generating crossing data\n",
            "Random sample currently exists.\n",
            " Loading from /content/drive/MyDrive/datasets/PIE/data_cache/random_samples.pkl\n",
            "The ratios are [0.5, 0.4, 0.1]\n",
            "Number of val tracks 178\n",
            "Subset: val\n",
            "Number of pedestrians: 178 \n",
            "Total number of samples: 166 \n",
            "---------------------------------------------------------\n",
            "Generating trajectory sequence data\n",
            "fstride: 1\n",
            "sample_type: all\n",
            "height_rng: [0, inf]\n",
            "squarify_ratio: 0\n",
            "data_split_type: random\n",
            "seq_type: crossing\n",
            "min_track_size: 61\n",
            "random_params: {'ratios': None, 'val_data': True, 'regen_data': False}\n",
            "kfold_params: {'num_folds': 5, 'fold': 1}\n",
            "subset: default\n",
            "---------------------------------------------------------\n",
            "Generating database for pie\n",
            "pie annotations loaded from /content/drive/MyDrive/datasets/PIE/data_cache/pie_database.pkl\n",
            "---------------------------------------------------------\n",
            "Generating crossing data\n",
            "Random sample currently exists.\n",
            " Loading from /content/drive/MyDrive/datasets/PIE/data_cache/random_samples.pkl\n",
            "The ratios are [0.5, 0.4, 0.1]\n",
            "Number of test tracks 712\n",
            "Subset: test\n",
            "Number of pedestrians: 712 \n",
            "Total number of samples: 661 \n"
          ]
        }
      ],
      "source": [
        "%cd '/content/drive/MyDrive/obj-3'\n",
        "import sys\n",
        "from vmi import VMI\n",
        "sys.path.insert(0,'/content/drive/MyDrive/datasets/PIE')\n",
        "from pie_data2 import PIE\n",
        "\n",
        "data_opts ={'fstride': 1,\n",
        "            'subset': 'default',\n",
        "            'data_split_type': 'random',  # kfold, random, default\n",
        "            'seq_type': 'crossing',\n",
        "            'min_track_size': 61} ## for obs length of 15 frames + 60 frames tte. This should be adjusted for different setup\n",
        "imdb = PIE(data_path='/content/drive/MyDrive/datasets/PIE') # change with the path to the dataset\n",
        "\n",
        "model_opts = {'obs_input_type': ['seg_box'],\n",
        "              'enlarge_ratio': 1.5,\n",
        "              'pred_target_type': ['trajectory'],\n",
        "              'obs_length': 15,  # Determines min track size\n",
        "              'time_to_event': 45, # Determines min track size\n",
        "              'dataset': 'pie',\n",
        "              'normalize_boxes': False}\n",
        "\n",
        "method_class = VMI()\n",
        "beh_seq_train = imdb.generate_data_trajectory_sequence('train', **data_opts)\n",
        "beh_seq_val = imdb.generate_data_trajectory_sequence('val', **data_opts)\n",
        "beh_seq_test = imdb.generate_data_trajectory_sequence('test', **data_opts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTUxNb4da8sD",
        "outputId": "e629476a-1ced-4aa0-850d-bcbc8627b95e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['image', 'pid', 'bbox', 'center', 'occlusion', 'age', 'gen', 'obd_speed', 'gps_speed', 'heading_angle', 'gps_coord', 'yrp', 'activities', 'Num_lanes', 'signalized', 'traffic_direction', 'intersection'])"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "beh_seq_train.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIyq12PxAUOS"
      },
      "source": [
        "#Image Preprocessing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fm7dj8LvaEIr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def squarify(bbox, squarify_ratio, img_width):\n",
        "    \"\"\"\n",
        "    Changes the ratio of bounding boxes to a fixed ratio\n",
        "    :param bbox: Bounding box tensor [x_min, y_min, x_max, y_max]\n",
        "    :param squarify_ratio: Ratio to be changed to\n",
        "    :param img_width: Image width tensor\n",
        "    :return: Squarified bounding box tensor [x_min, y_min, x_max, y_max]\n",
        "    \"\"\"\n",
        "    width = tf.abs(bbox[0] - bbox[2])\n",
        "    height = tf.abs(bbox[1] - bbox[3])\n",
        "    width_change = height * squarify_ratio - width\n",
        "    bbox_x_min = bbox[0] - width_change / 2\n",
        "    bbox_x_max = bbox[2] + width_change / 2\n",
        "\n",
        "    # Cast img_width to float64\n",
        "    img_width = tf.cast(img_width, tf.float64)\n",
        "\n",
        "    # Clamp bounding box to image borders\n",
        "    bbox_x_min = tf.maximum(bbox_x_min, 0)\n",
        "    bbox_x_max = tf.minimum(bbox_x_max, img_width)\n",
        "\n",
        "    # Concatenate x_min, y_min, x_max, y_max into a tensor\n",
        "    squarified_bbox = tf.stack([bbox_x_min, bbox[1], bbox_x_max, bbox[3]])\n",
        "\n",
        "    return squarified_bbox\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxet9nRVh7jI"
      },
      "outputs": [],
      "source": [
        "def convert_normalize_bboxes(all_bboxes, normalize, bbox_type):\n",
        "    '''input box type is x1y1x2y2 in original resolution'''\n",
        "    for i in range(len(all_bboxes)):\n",
        "        if len(all_bboxes[i]) == 0:\n",
        "            continue\n",
        "        bbox = np.array(all_bboxes[i])\n",
        "        # NOTE ltrb to cxcywh\n",
        "        if bbox_type == 'cxcywh':\n",
        "            bbox[..., [2, 3]] = bbox[..., [2, 3]] - bbox[..., [0, 1]]\n",
        "            bbox[..., [0, 1]] += bbox[..., [2, 3]]/2\n",
        "        # NOTE Normalize bbox\n",
        "        if normalize == 'zero-one':\n",
        "            # W, H  = all_resolutions[i][0]\n",
        "            _min = np.array(self.args.min_bbox)[None, :]\n",
        "            _max = np.array(self.args.max_bbox)[None, :]\n",
        "            bbox = (bbox - _min) / (_max - _min)\n",
        "        elif normalize == 'plus-minus-one':\n",
        "            # W, H  = all_resolutions[i][0]\n",
        "            _min = np.array(self.args.min_bbox)[None, :]\n",
        "            _max = np.array(self.args.max_bbox)[None, :]\n",
        "            bbox = (2 * (bbox - _min) / (_max - _min)) - 1\n",
        "        elif normalize == 'none':\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(normalize)\n",
        "        all_bboxes[i] = bbox\n",
        "    return all_bboxes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TN6WWDc9GVK9"
      },
      "outputs": [],
      "source": [
        "def bbox_sanity_check(img_size, bbox):\n",
        "\t\"\"\"\n",
        "\tConfirms that the bounding boxes are within image boundaries.\n",
        "\tIf this is not the case, modifications is applied.\n",
        "\t:param img_size: The size of the image\n",
        "\t:param bbox: The bounding box coordinates\n",
        "\t:return: The modified/original bbox\n",
        "\t\"\"\"\n",
        "\timg_width, img_heigth = img_size\n",
        "\tif bbox[0] < 0:\n",
        "\t\tbbox[0] = 0.0\n",
        "\tif bbox[1] < 0:\n",
        "\t\tbbox[1] = 0.0\n",
        "\tif bbox[2] >= img_width:\n",
        "\t\tbbox[2] = img_width - 1\n",
        "\tif bbox[3] >= img_heigth:\n",
        "\t\tbbox[3] = img_heigth - 1\n",
        "\treturn bbox\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oQ4AeSAGQXK"
      },
      "outputs": [],
      "source": [
        "def squarify(bbox, squarify_ratio, img_width):\n",
        "\t\"\"\"\n",
        "\tChanges is the ratio of bounding boxes to a fixed ratio\n",
        "\t:param bbox: Bounding box\n",
        "\t:param squarify_ratio: Ratio to be changed to\n",
        "\t:param img_width: Image width\n",
        "\t:return: Squarified boduning box\n",
        "\t\"\"\"\n",
        "\twidth = abs(bbox[0] - bbox[2])\n",
        "\theight = abs(bbox[1] - bbox[3])\n",
        "\twidth_change = height * squarify_ratio - width\n",
        "\tbbox[0] = bbox[0] - width_change/2\n",
        "\tbbox[2] = bbox[2] + width_change/2\n",
        "\t# Squarify is applied to bounding boxes in Matlab coordinate starting from 1\n",
        "\tif bbox[0] < 0:\n",
        "\t\tbbox[0] = 0\n",
        "\n",
        "\t# check whether the new bounding box goes beyond image boarders\n",
        "\t# If this is the case, the bounding box is shifted back\n",
        "\tif bbox[2] > img_width:\n",
        "\t\t# bbox[1] = str(-float(bbox[3]) + img_dimensions[0])\n",
        "\t\tbbox[0] = bbox[0]-bbox[2] + img_width\n",
        "\t\tbbox[2] = img_width\n",
        "\treturn bbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6BwJlPtGLLK"
      },
      "outputs": [],
      "source": [
        "\n",
        "def jitter_bbox(img_path, bbox, mode, ratio):\n",
        "\t\"\"\"\n",
        "\tThis method jitters the position or dimensions of the bounding box.\n",
        "\t:param img_path: The to the image\n",
        "\t:param bbox: The bounding box to be jittered\n",
        "\t:param mode: The mode of jittere:\n",
        "\t'same' returns the bounding box unchanged\n",
        "\t\t  'enlarge' increases the size of bounding box based on the given ratio.\n",
        "\t\t  'random_enlarge' increases the size of bounding box by randomly sampling a value in [0,ratio)\n",
        "\t\t  'move' moves the center of the bounding box in each direction based on the given ratio\n",
        "\t\t  'random_move' moves the center of the bounding box in each direction by randomly\n",
        "\t\t\t\t\t\tsampling a value in [-ratio,ratio)\n",
        "\t:param ratio: The ratio of change relative to the size of the bounding box.\n",
        "\t\t   For modes 'enlarge' and 'random_enlarge'\n",
        "\t\t   the absolute value is considered.\n",
        "\t:return: Jittered bounding box\n",
        "\t\"\"\"\n",
        "\n",
        "\tassert(mode in ['same','enlarge','move','random_enlarge','random_move']), \\\n",
        "\t\t\t'mode %s is invalid.' % mode\n",
        "\n",
        "\tif mode == 'same':\n",
        "\t\treturn bbox\n",
        "\n",
        "\timg = load_img(img_path)\n",
        "\n",
        "\tif mode in ['random_enlarge', 'enlarge']:\n",
        "\t\tjitter_ratio  = abs(ratio)\n",
        "\telse:\n",
        "\t\tjitter_ratio  = ratio\n",
        "\n",
        "\tif mode == 'random_enlarge':\n",
        "\t\tjitter_ratio = np.random.random_sample()*jitter_ratio\n",
        "\telif mode == 'random_move':\n",
        "\t\t# for ratio between (-jitter_ratio, jitter_ratio)\n",
        "\t\t# for sampling the formula is [a,b), b > a,\n",
        "\t\t# random_sample * (b-a) + a\n",
        "\t\tjitter_ratio = np.random.random_sample() * jitter_ratio * 2 - jitter_ratio\n",
        "\n",
        "\tjit_boxes = []\n",
        "\tfor b in bbox:\n",
        "\t\tbbox_width = b[2] - b[0]\n",
        "\t\tbbox_height = b[3] - b[1]\n",
        "\n",
        "\t\twidth_change = bbox_width * jitter_ratio\n",
        "\t\theight_change = bbox_height * jitter_ratio\n",
        "\n",
        "\t\tif width_change < height_change:\n",
        "\t\t\theight_change = width_change\n",
        "\t\telse:\n",
        "\t\t\twidth_change = height_change\n",
        "\n",
        "\t\tif mode in ['enlarge','random_enlarge']:\n",
        "\t\t\tb[0] = b[0] - width_change //2\n",
        "\t\t\tb[1] = b[1] - height_change //2\n",
        "\t\telse:\n",
        "\t\t\tb[0] = b[0] + width_change //2\n",
        "\t\t\tb[1] = b[1] + height_change //2\n",
        "\n",
        "\t\tb[2] = b[2] + width_change //2\n",
        "\t\tb[3] = b[3] + height_change //2\n",
        "\n",
        "\t\t# Checks to make sure the bbox is not exiting the image boundaries\n",
        "\t\tb = bbox_sanity_check(img.size, b)\n",
        "\t\tjit_boxes.append(b)\n",
        "\t# elif crop_opts['mode'] == 'border_only':\n",
        "\treturn jit_boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PU9uKm6eJ2PD"
      },
      "outputs": [],
      "source": [
        "def img_pad(img, mode = 'warp', size = 224):\n",
        "\t\"\"\"\n",
        "\tPads a image given the boundries of the box needed\n",
        "\t:param img: The image to be coropped and/or padded\n",
        "\t:param mode: The type of padding or resizing:\n",
        "\t\t\twarp: crops the bounding box and resize to the output size\n",
        "\t\t\tsame: only crops the image\n",
        "\t\t\tpad_same: maintains the original size of the cropped box  and pads with zeros\n",
        "\t\t\tpad_resize: crops the image and resize the cropped box in a way that the longer edge is equal to\n",
        "\t\t\t\t\t\tthe desired output size in that direction while maintaining the aspect ratio. The rest\n",
        "\t\t\t\t\t\tof the image is\tpadded with zeros\n",
        "\t\t\tpad_fit: maintains the original size of the cropped box unless the image is bigger than the size\n",
        "\t\t\t\t\tin which case it scales the image down, and then pads it\n",
        "\t:param size: Target size of image\n",
        "\t:return:\n",
        "\t\"\"\"\n",
        "\tassert(mode in ['same', 'warp', 'pad_same', 'pad_resize', 'pad_fit']), 'Pad mode %s is invalid' % mode\n",
        "\timage = img.copy()\n",
        "\tif mode == 'warp':\n",
        "\t\twarped_image = image.resize((size,size), PIL.Image.NEAREST)\n",
        "\t\treturn warped_image\n",
        "\telif mode == 'same':\n",
        "\t\treturn image\n",
        "\telif mode in ['pad_same', 'pad_resize', 'pad_fit']:\n",
        "\t\timg_size = image.size  # size is in (width, height)\n",
        "\t\tratio = float(size)/max(img_size)\n",
        "\t\tif mode == 'pad_resize' or\t\\\n",
        "\t\t\t(mode == 'pad_fit' and (img_size[0] > size or img_size[1] > size)):\n",
        "\t\t\timg_size = tuple([int(img_size[0]*ratio),int(img_size[1]*ratio)])\n",
        "\t\t\timage = image.resize(img_size, PIL.Image.NEAREST)\n",
        "\t\tpadded_image = PIL.Image.new(\"RGB\", (size, size))\n",
        "\t\tpadded_image.paste(image, ((size-img_size [0])//2,\n",
        "\t\t\t\t\t(size-img_size [1])//2))\n",
        "\t\treturn padded_image\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sgTUCzs8srX"
      },
      "outputs": [],
      "source": [
        "def get_traj_tracks(dataset, data_types, observe_length, predict_length, overlap, normalize):\n",
        "        \"\"\"\n",
        "        Generates tracks by sampling from pedestrian sequences\n",
        "        :param dataset: The raw data passed to the method\n",
        "        :param data_types: Specification of types of data for encoder and decoder. Data types depend on datasets. e.g.\n",
        "        JAAD has 'bbox', 'ceneter' and PIE in addition has 'obd_speed', 'heading_angle', etc.\n",
        "        :param observe_length: The length of the observation (i.e. time steps of the encoder)\n",
        "        :param predict_length: The length of the prediction (i.e. time steps of the decoder)\n",
        "        :param overlap: How much the sampled tracks should overlap. A value between [0,1) should be selected\n",
        "        :param normalize: Whether to normalize center/bounding box coordinates, i.e. convert to velocities. NOTE: when\n",
        "        the tracks are normalized, observation length becomes 1 step shorter, i.e. first step is removed.\n",
        "        :return: A dictinary containing sampled tracks for each data modality\n",
        "        \"\"\"\n",
        "        #  Calculates the overlap in terms of number of frames\n",
        "        seq_length = observe_length + predict_length\n",
        "        overlap_stride = observe_length if overlap == 0 else \\\n",
        "            int((1 - overlap) * observe_length)\n",
        "        overlap_stride = 1 if overlap_stride < 1 else overlap_stride\n",
        "\n",
        "        #  Check the validity of keys selected by user as data type\n",
        "        d = {}\n",
        "        for dt in data_types:\n",
        "            try:\n",
        "                d[dt] = dataset[dt]\n",
        "            except:# KeyError:\n",
        "                raise KeyError('Wrong data type is selected %s' % dt)\n",
        "\n",
        "        d['image'] = dataset['image']\n",
        "\n",
        "        d['pid'] = dataset['pid']\n",
        "        # d['resolution'] = dataset['resolution']\n",
        "        # d['flow'] = []\n",
        "        num_trks = len(d['image'])\n",
        "        #  Sample tracks from sequneces\n",
        "        for k in d.keys():\n",
        "            tracks = []\n",
        "            for track in d[k]:\n",
        "                for i in range(0, len(track) - seq_length + 1, overlap_stride):\n",
        "                    tracks.append(track[i:i + seq_length])\n",
        "            d[k] = tracks\n",
        "        #  Normalize tracks using FOL paper method,\n",
        "        d['bbox'] = convert_normalize_bboxes(d['bbox'],'none', 'ltrb')\n",
        "        d['intention']= dataset['activities']\n",
        "        return d\n",
        "\n",
        "def get_traj_data(data, **model_opts):\n",
        "    \"\"\"\n",
        "    Main data generation function for training/testing\n",
        "    :param data: The raw data\n",
        "    :param model_opts: Control parameters for data generation characteristics (see below for default values)\n",
        "    :return: A dictionary containing training and testing data\n",
        "    \"\"\"\n",
        "\n",
        "    opts = {\n",
        "        'normalize_bbox': True,\n",
        "        'track_overlap': 0.5,\n",
        "        'observe_length': 15,\n",
        "        'predict_length': 45,\n",
        "        'enc_input_type': ['bbox'],\n",
        "        'dec_input_type': [],\n",
        "        'prediction_type': ['bbox']\n",
        "    }\n",
        "    for key, value in model_opts.items():\n",
        "        assert key in opts.keys(), 'wrong data parameter %s' % key\n",
        "        opts[key] = value\n",
        "\n",
        "    observe_length = opts['observe_length']\n",
        "    data_types = set(opts['enc_input_type'] + opts['dec_input_type'] + opts['prediction_type'])\n",
        "    data_tracks = get_traj_tracks(data, data_types, observe_length,\n",
        "                                  opts['predict_length'], opts['track_overlap'],\n",
        "                                  opts['normalize_bbox'])\n",
        "    obs_slices = {}\n",
        "    pred_slices = {}\n",
        "    #  Generate observation/prediction sequences from the tracks\n",
        "    for k in data_tracks.keys():\n",
        "        obs_slices[k] = []\n",
        "        pred_slices[k] = []\n",
        "        # NOTE: Add downsample function\n",
        "        down = 1\n",
        "        obs_slices[k].extend([d[down-1:observe_length:down] for d in data_tracks[k]])\n",
        "        pred_slices[k].extend([d[observe_length+down-1::down] for d in data_tracks[k]])\n",
        "\n",
        "    ret =  {'obs_image': obs_slices['image'],\n",
        "            'obs_pid': obs_slices['pid'],\n",
        "\n",
        "            'pred_image': pred_slices['image'],\n",
        "            'pred_pid': pred_slices['pid'],\n",
        "\n",
        "\n",
        "            'obs_bbox': np.array(obs_slices['bbox']), #enc_input,\n",
        "\n",
        "            'pred_bbox': np.array(pred_slices['bbox']), #pred_target,\n",
        "            'intent': obs_slices['intention']\n",
        "            }\n",
        "\n",
        "    return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vH53mE1zHtdn"
      },
      "outputs": [],
      "source": [
        "traj_model_opts = {'normalize_bbox': True,\n",
        "              'track_overlap': 0.5,\n",
        "              'observe_length': 15,\n",
        "              'predict_length': 45,\n",
        "              'enc_input_type': ['bbox'],\n",
        "              'dec_input_type': [],\n",
        "              'prediction_type': ['bbox']\n",
        "              }\n",
        "\n",
        "traj_model_opts['enc_input_type'].extend(['obd_speed', 'heading_angle'])\n",
        "traj_model_opts['prediction_type'].extend(['obd_speed', 'heading_angle'])\n",
        "\n",
        "train_data = get_traj_data(beh_seq_train, **traj_model_opts)\n",
        "test_data = get_traj_data(beh_seq_test, **traj_model_opts)\n",
        "val_data = get_traj_data(beh_seq_val, **traj_model_opts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lU0wcirlGzGC"
      },
      "outputs": [],
      "source": [
        "# !rm -r '/content/drive/MyDrive/obj-3/data/features/pie/local_box'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qyufo-Z-dk3C"
      },
      "outputs": [],
      "source": [
        "def get_traj_tracks(dataset, data_types, observe_length, predict_length, overlap, normalize):\n",
        "        \"\"\"\n",
        "        Generates tracks by sampling from pedestrian sequences\n",
        "        :param dataset: The raw data passed to the method\n",
        "        :param data_types: Specification of types of data for encoder and decoder. Data types depend on datasets. e.g.\n",
        "        JAAD has 'bbox', 'ceneter' and PIE in addition has 'obd_speed', 'heading_angle', etc.\n",
        "        :param observe_length: The length of the observation (i.e. time steps of the encoder)\n",
        "        :param predict_length: The length of the prediction (i.e. time steps of the decoder)\n",
        "        :param overlap: How much the sampled tracks should overlap. A value between [0,1) should be selected\n",
        "        :param normalize: Whether to normalize center/bounding box coordinates, i.e. convert to velocities. NOTE: when\n",
        "        the tracks are normalized, observation length becomes 1 step shorter, i.e. first step is removed.\n",
        "        :return: A dictinary containing sampled tracks for each data modality\n",
        "        \"\"\"\n",
        "        #  Calculates the overlap in terms of number of frames\n",
        "        seq_length = observe_length + predict_length\n",
        "        overlap_stride = observe_length if overlap == 0 else \\\n",
        "            int((1 - overlap) * observe_length)\n",
        "        overlap_stride = 1 if overlap_stride < 1 else overlap_stride\n",
        "\n",
        "        #  Check the validity of keys selected by user as data type\n",
        "        d = {}\n",
        "        for dt in data_types:\n",
        "            try:\n",
        "                d[dt] = dataset[dt]\n",
        "            except:# KeyError:\n",
        "                raise KeyError('Wrong data type is selected %s' % dt)\n",
        "\n",
        "        d['image'] = dataset['image']\n",
        "        d['pid'] = dataset['pid']\n",
        "        d['age'] = dataset['age']\n",
        "        d['gen'] = dataset['gen']\n",
        "        d['signalized'] = dataset['signalized']\n",
        "        d['Num_lanes'] = dataset['Num_lanes']\n",
        "        d['activities'] = dataset['activities']\n",
        "        # d['resolution'] = dataset['resolution']\n",
        "        # d['flow'] = []\n",
        "        num_trks = len(d['image'])\n",
        "        #  Sample tracks from sequneces\n",
        "        for k in d.keys():\n",
        "            tracks = []\n",
        "            for track in d[k]:\n",
        "                for i in range(0, len(track) - seq_length + 1, overlap_stride):\n",
        "                    tracks.append(track[i:i + seq_length])\n",
        "            d[k] = tracks\n",
        "        #  Normalize tracks using FOL paper method,\n",
        "        d['bbox'] = convert_normalize_bboxes(d['bbox'],'none', 'ltrb')\n",
        "        return d\n",
        "\n",
        "def get_traj_data(data, **model_opts):\n",
        "    \"\"\"\n",
        "    Main data generation function for training/testing\n",
        "    :param data: The raw data\n",
        "    :param model_opts: Control parameters for data generation characteristics (see below for default values)\n",
        "    :return: A dictionary containing training and testing data\n",
        "    \"\"\"\n",
        "\n",
        "    opts = {\n",
        "        'normalize_bbox': True,\n",
        "        'track_overlap': 0.5,\n",
        "        'observe_length': 15,\n",
        "        'predict_length': 45,\n",
        "        'enc_input_type': ['bbox'],\n",
        "        'dec_input_type': [],\n",
        "        'prediction_type': ['bbox']\n",
        "    }\n",
        "    for key, value in model_opts.items():\n",
        "        assert key in opts.keys(), 'wrong data parameter %s' % key\n",
        "        opts[key] = value\n",
        "\n",
        "    observe_length = opts['observe_length']\n",
        "    data_types = set(opts['enc_input_type'] + opts['dec_input_type'] + opts['prediction_type'])\n",
        "    data_tracks = get_traj_tracks(data, data_types, observe_length,\n",
        "                                  opts['predict_length'], opts['track_overlap'],\n",
        "                                  opts['normalize_bbox'])\n",
        "    obs_slices = {}\n",
        "    pred_slices = {}\n",
        "    #  Generate observation/prediction sequences from the tracks\n",
        "    for k in data_tracks.keys():\n",
        "        obs_slices[k] = []\n",
        "        pred_slices[k] = []\n",
        "        # NOTE: Add downsample function\n",
        "        down = 1\n",
        "        obs_slices[k].extend([d[down-1:observe_length:down] for d in data_tracks[k]])\n",
        "        pred_slices[k].extend([d[observe_length+down-1::down] for d in data_tracks[k]])\n",
        "\n",
        "    ret =  {'obs_image': obs_slices['image'],\n",
        "            'obs_pid': obs_slices['pid'],\n",
        "\n",
        "            'pred_image': pred_slices['image'],\n",
        "            'pred_pid': pred_slices['pid'],\n",
        "\n",
        "\n",
        "            'obs_bbox': np.array(obs_slices['bbox']), #enc_input,\n",
        "\n",
        "            'pred_bbox': np.array(pred_slices['bbox']), #pred_target,\n",
        "            'age': np.array(obs_slices['age']),\n",
        "            'gen': np.array(obs_slices['gen']),\n",
        "            'signalized': np.array(obs_slices['signalized']),\n",
        "            'lanes': np.array(obs_slices['Num_lanes']),\n",
        "            'activities': np.array(obs_slices['activities'])\n",
        "            }\n",
        "\n",
        "    return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPJQE4q8dovU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "traj_model_opts = {'normalize_bbox': True,\n",
        "              'track_overlap': 0.5,\n",
        "              'observe_length': 15,\n",
        "              'predict_length': 45,\n",
        "              'enc_input_type': ['bbox'],\n",
        "              'dec_input_type': [],\n",
        "              'prediction_type': ['bbox']\n",
        "              }\n",
        "\n",
        "traj_model_opts['enc_input_type'].extend(['obd_speed', 'heading_angle'])\n",
        "traj_model_opts['prediction_type'].extend(['obd_speed', 'heading_angle'])\n",
        "\n",
        "train_data = get_traj_data(beh_seq_train, **traj_model_opts)\n",
        "# val_data = get_traj_data(beh_seq_val, **traj_model_opts)\n",
        "# test_data = get_traj_data(beh_seq_test, **traj_model_opts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esaDux2pdtOs",
        "outputId": "1834515b-366d-4ca6-e9e1-b23af7163564"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['obs_image', 'obs_pid', 'pred_image', 'pred_pid', 'obs_bbox', 'pred_bbox', 'age', 'gen', 'signalized', 'lanes', 'activities'])"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo1Y30Z4TIYl"
      },
      "source": [
        "#Custom Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oho8ntsRD738"
      },
      "outputs": [],
      "source": [
        "#Custom Data Generator\n",
        "save_path1 = '/content/drive/MyDrive/obj4/features'\n",
        "save_path2 = '/content/drive/MyDrive/obj4/seg_features'\n",
        "# convnet = tf.keras.applications.efficientnet.EfficientNetB4(input_shape=(224, 224, 3),\n",
        "#                               include_top=False, weights='imagenet')\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import PIL\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "class CustomDataGen(tf.keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self,x,y,\n",
        "                 batch_size,\n",
        "                 input_size=(224, 224, 3),\n",
        "                 shuffle=True, train=1):\n",
        "\n",
        "\n",
        "        self.x=x\n",
        "        self.y =y\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.input_size = input_size\n",
        "        self.shuffle = shuffle\n",
        "        self.train= train\n",
        "        self.n = len(self.x[0])\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            ind = np.random.randint(0, len(self.x[0]), len(self.x[0]))\n",
        "            self.x[0] = self.x[0][ind]\n",
        "            self.x[1] = self.x[1][ind]\n",
        "            self.x[2] = self.x[2][ind]\n",
        "            self.y = self.y[ind]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def __get_input(self, path_batch, bbox, target_size, seg=0):\n",
        "\n",
        "\n",
        "      if seg==0:\n",
        "        img_seq=[]\n",
        "        for path,b in zip(path_batch,bbox):\n",
        "\n",
        "           img_seq.append(path)\n",
        "\n",
        "\n",
        "\n",
        "      elif seg==1:\n",
        "\n",
        "        img_seq=[]\n",
        "        for path,b in zip(path_batch,bbox):\n",
        "           seg_path= path.replace(\"images\", \"seg_images\")\n",
        "           img_seq.append(seg_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      else:\n",
        "\n",
        "          img_seq=[]\n",
        "            # Calculate minimum and maximum values\n",
        "\n",
        "          x_max = 1920\n",
        "          y_max = 1080\n",
        "\n",
        "\n",
        "            # Normalize the bounding box sequence\n",
        "          for b in bbox:\n",
        "            img_seq.append(b)\n",
        "\n",
        "              # x_bleft, y_bleft, x_tright, y_tright = b\n",
        "              # xbl_normalized = (x_bleft) / (x_max)\n",
        "              # ybl_normalized = (y_bleft) / (y_max)\n",
        "              # xtr_normalized = (x_tright) / (x_max)\n",
        "              # ytr_normalized = (y_tright) / (y_max)\n",
        "\n",
        "\n",
        "\n",
        "              # normalized_b = (\n",
        "              #     xbl_normalized,\n",
        "              #     ybl_normalized,\n",
        "              #     xtr_normalized,\n",
        "              #     ytr_normalized\n",
        "              # )\n",
        "\n",
        "\n",
        "          #     box_seq.append(normalized_b)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      return img_seq\n",
        "\n",
        "\n",
        "    def __get_data(self, batches_b, batches_p, batches_i):\n",
        "        # # Generates data containing batch_size samples\n",
        "\n",
        "\n",
        "        bbox_batch = batches_b\n",
        "        pred_batch = batches_p\n",
        "        image_batch = batches_i\n",
        "\n",
        "        x_im_bat=[]\n",
        "        x_seg_bat=[]\n",
        "        x_bbox_bat=[]\n",
        "        x_pred_bat=[]\n",
        "\n",
        "\n",
        "        for i,j in zip(image_batch, bbox_batch):\n",
        "\n",
        "\n",
        "          x_bbox_bat.append(self.__get_input(i, j, self.input_size, seg=2))\n",
        "        X_bbox_batch =np.asarray(x_bbox_bat)\n",
        "\n",
        "\n",
        "\n",
        "        for i,j in zip(image_batch, pred_batch):\n",
        "\n",
        "\n",
        "          x_pred_bat.append(self.__get_input(i, j, self.input_size, seg=2))\n",
        "        X_pred_batch =np.asarray(x_pred_bat)\n",
        "\n",
        "        for i,j in zip(image_batch, bbox_batch):\n",
        "\n",
        "          inp = self.__get_input(i, j, self.input_size, seg=0)\n",
        "          x_im_bat.append(inp)\n",
        "\n",
        "        X_batch =np.asarray(x_im_bat)\n",
        "\n",
        "        for i,j in zip(image_batch, bbox_batch):\n",
        "          inp = self.__get_input(i, j, self.input_size, seg=1)\n",
        "          x_seg_bat.append(inp)\n",
        "        X_seg_batch =np.asarray(x_seg_bat)\n",
        "\n",
        "\n",
        "        return tf.squeeze(X_bbox_batch, axis=0), tf.squeeze(X_pred_batch, axis=0), tf.squeeze(X_batch, axis=0), tf.squeeze(X_seg_batch, axis=0)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "\n",
        "        batches_obs = self.x[0][index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        batches_pred =  self.x[1][index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        batches_im = self.x[2][index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        batches_y = self.y[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        X_obs_box, X_pred_box, X, X_seg = self.__get_data(batches_obs, batches_pred, batches_im)\n",
        "\n",
        "\n",
        "        return tf.convert_to_tensor(X), tf.convert_to_tensor(X_seg), tf.convert_to_tensor(X_obs_box), tf.convert_to_tensor(X_pred_box)\n",
        "\n",
        "    def __call__(self):\n",
        "        for i in range(self.__len__()):\n",
        "            yield self.__getitem__(i)\n",
        "\n",
        "            if i == self.__len__()-1:\n",
        "                self.on_epoch_end()\n",
        "    def __len__(self):\n",
        "        return self.n // self.batch_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JDL7d1wIgIl"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE=8\n",
        "x_train = [train_data['obs_bbox'], train_data['pred_bbox'], train_data['obs_image']]\n",
        "y_train = train_data['activities']\n",
        "# x_val = [val_data['obs_bbox'], val_data['pred_bbox'], val_data['obs_image']]\n",
        "# x_test = [test_data['obs_bbox'], test_data['pred_bbox'], test_data['obs_image']]\n",
        "traingen = CustomDataGen(x_train,y_train, batch_size=1)\n",
        "# valgen = CustomDataGen([x_val[0], x_val[1]],batch_size=BATCH_SIZE)\n",
        "# testgen = CustomDataGen([x_test[0], x_test[1]], batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlmwmqJRd02m",
        "outputId": "836dc97e-887d-4c87-bfbc-fa65f6410b0e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(15,), dtype=string, numpy=\n",
              " array([b'/content/drive/MyDrive/datasets/PIE/images/set01/video_0001/01013.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/images/set01/video_0001/01014.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/images/set01/video_0001/01015.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/images/set01/video_0001/01016.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/images/set01/video_0001/01017.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/images/set01/video_0001/01018.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/images/set01/video_0001/01019.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/images/set01/video_0001/01020.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/images/set01/video_0001/01021.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/images/set01/video_0001/01022.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/images/set01/video_0001/01023.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/images/set01/video_0001/01024.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/images/set01/video_0001/01025.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/images/set01/video_0001/01026.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/images/set01/video_0001/01027.png'],\n",
              "       dtype=object)>,\n",
              " <tf.Tensor: shape=(15,), dtype=string, numpy=\n",
              " array([b'/content/drive/MyDrive/datasets/PIE/seg_images/set01/video_0001/01013.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/seg_images/set01/video_0001/01014.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/seg_images/set01/video_0001/01015.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/seg_images/set01/video_0001/01016.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/seg_images/set01/video_0001/01017.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/seg_images/set01/video_0001/01018.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/seg_images/set01/video_0001/01019.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/seg_images/set01/video_0001/01020.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/seg_images/set01/video_0001/01021.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/seg_images/set01/video_0001/01022.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/seg_images/set01/video_0001/01023.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/seg_images/set01/video_0001/01024.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/seg_images/set01/video_0001/01025.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/seg_images/set01/video_0001/01026.png',\n",
              "        b'/content/drive/MyDrive/datasets/PIE/seg_images/set01/video_0001/01027.png'],\n",
              "       dtype=object)>,\n",
              " <tf.Tensor: shape=(15, 4), dtype=float64, numpy=\n",
              " array([[916.82, 793.32, 929.18, 847.11],\n",
              "        [920.92, 792.1 , 933.54, 850.51],\n",
              "        [924.42, 796.04, 937.32, 853.9 ],\n",
              "        [927.93, 799.98, 941.09, 857.3 ],\n",
              "        [931.44, 802.09, 946.4 , 858.5 ],\n",
              "        [937.3 , 807.86, 951.  , 864.1 ],\n",
              "        [942.08, 813.1 , 954.54, 868.25],\n",
              "        [943.12, 815.67, 957.1 , 871.6 ],\n",
              "        [946.29, 818.38, 961.3 , 874.7 ],\n",
              "        [949.46, 821.09, 963.82, 877.52],\n",
              "        [952.64, 823.8 , 966.34, 880.33],\n",
              "        [954.76, 826.  , 969.2 , 883.6 ],\n",
              "        [958.85, 826.31, 974.1 , 884.32],\n",
              "        [962.95, 826.62, 979.  , 885.04],\n",
              "        [967.04, 826.93, 983.9 , 885.76]])>,\n",
              " <tf.Tensor: shape=(45, 4), dtype=float64, numpy=\n",
              " array([[ 970.16,  829.82,  987.14,  889.05],\n",
              "        [ 975.22,  827.55,  992.3 ,  887.2 ],\n",
              "        [ 979.98,  828.27,  998.3 ,  887.91],\n",
              "        [ 984.37,  826.78, 1002.38,  886.42],\n",
              "        [ 988.75,  825.29, 1006.45,  884.94],\n",
              "        [ 993.13,  826.17, 1010.52,  885.81],\n",
              "        [ 999.01,  831.11, 1016.09,  890.76],\n",
              "        [1002.99,  827.02, 1020.3 ,  887.38],\n",
              "        [1007.61,  825.99, 1025.15,  887.06],\n",
              "        [1012.23,  824.95, 1030.  ,  886.74],\n",
              "        [1016.85,  823.91, 1034.85,  886.42],\n",
              "        [1021.47,  822.88, 1039.7 ,  886.1 ],\n",
              "        [1026.23,  822.35, 1044.68,  886.68],\n",
              "        [1030.99,  821.82, 1049.66,  887.26],\n",
              "        [1035.75,  821.28, 1054.64,  887.84],\n",
              "        [1040.51,  820.75, 1059.62,  888.42],\n",
              "        [1045.27,  820.22, 1064.6 ,  889.  ],\n",
              "        [1050.83,  820.41, 1070.15,  889.5 ],\n",
              "        [1056.38,  820.6 , 1075.7 ,  890.  ],\n",
              "        [1061.93,  820.78, 1081.25,  890.51],\n",
              "        [1067.49,  820.97, 1086.8 ,  891.01],\n",
              "        [1072.4 ,  819.55, 1091.7 ,  889.9 ],\n",
              "        [1077.47,  818.97, 1097.04,  890.22],\n",
              "        [1082.54,  818.39, 1102.38,  890.54],\n",
              "        [1087.61,  817.82, 1107.72,  890.86],\n",
              "        [1092.68,  817.24, 1113.06,  891.18],\n",
              "        [1097.75,  816.66, 1118.4 ,  891.5 ],\n",
              "        [1102.55,  817.1 , 1123.12,  892.44],\n",
              "        [1107.36,  817.55, 1127.84,  893.38],\n",
              "        [1112.16,  817.99, 1132.56,  894.32],\n",
              "        [1116.97,  818.44, 1137.28,  895.26],\n",
              "        [1121.77,  818.88, 1142.  ,  896.2 ],\n",
              "        [1125.99,  818.57, 1146.26,  896.56],\n",
              "        [1130.22,  818.26, 1150.52,  896.92],\n",
              "        [1134.44,  817.95, 1154.78,  897.28],\n",
              "        [1138.67,  817.64, 1159.04,  897.64],\n",
              "        [1142.89,  817.33, 1163.3 ,  898.  ],\n",
              "        [1147.11,  816.77, 1168.14,  899.03],\n",
              "        [1151.34,  816.2 , 1172.98,  900.06],\n",
              "        [1155.56,  815.64, 1177.83,  901.1 ],\n",
              "        [1159.79,  815.07, 1182.67,  902.13],\n",
              "        [1164.01,  814.51, 1187.51,  903.16],\n",
              "        [1168.55,  813.7 , 1192.27,  902.66],\n",
              "        [1173.08,  812.9 , 1197.03,  902.16],\n",
              "        [1177.55,  811.41, 1201.72,  902.17]])>)"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "traingen[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrDHYfE9Zag4"
      },
      "outputs": [],
      "source": [
        "ot = tf.string, tf.string, tf.float64, tf.float64\n",
        "\n",
        "os = (None)\n",
        "ds = tf.data.Dataset.from_generator(traingen,\n",
        "                                    output_types = ot,\n",
        "                                    output_shapes = os)\n",
        "# ds = ds.prefetch(tf.data.AUTOTUNE).cache().batch(8, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bu_yVpSclUpm",
        "outputId": "d8aa6fac-92b7-4c28-f5b0-3c60f105c032"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(15,)\n"
          ]
        }
      ],
      "source": [
        "# Example usage of the dataset\n",
        "for z in ds.take(1):\n",
        "   a,b,c,d=z\n",
        "print(a.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wALgxUbUioGa"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "\n",
        "\n",
        "def preprocess_and_compute_features_img(a,g):\n",
        "    base_model = EfficientNetB0(include_top=False, weights='imagenet')\n",
        "    all_tensors_1 = tf.TensorArray(dtype=tf.float32, size=len(a), dynamic_size=True, clear_after_read=False)\n",
        "\n",
        "\n",
        "    for i in range(len(a)):\n",
        "        flip_condition = tf.strings.regex_full_match(a[i], \".*flip.*\")\n",
        "\n",
        "        image = tf.io.read_file(a[i])  # Read image file\n",
        "        image = tf.image.decode_png(image, channels=3)  # Decode PNG image\n",
        "        image = tf.cond(flip_condition, lambda: tf.image.flip_left_right(image), lambda: image)\n",
        "        try:\n",
        "              box = g[i]\n",
        "              # box = squarify(box, 1, tf.shape(image)[0])\n",
        "              box= tf.cast(box, tf.int32)\n",
        "              offset_height = box[1]\n",
        "              offset_width = box[0]\n",
        "              # target_height = tf.abs(box[3]-box[1])\n",
        "              # target_width = tf.abs(box[2]-box[0])\n",
        "              target_height = tf.maximum(tf.abs(box[3] - box[1]), 1)\n",
        "              target_width = tf.maximum(tf.abs(box[2] - box[0]), 1)\n",
        "\n",
        "              # Check if target_width is less than or equal to 0\n",
        "\n",
        "              cropped_image = tf.image.crop_to_bounding_box(image, offset_height, offset_width, target_height, target_width)\n",
        "        except Exception as e:\n",
        "              cropped_image = image\n",
        "        # # Resize or pad the cropped image to 224x224\n",
        "        cropped_image = tf.image.resize_with_pad(cropped_image, 224, 224)\n",
        "\n",
        "        # Preprocess image for EfficientNet\n",
        "        image = tf.cast(cropped_image, tf.float32)  # Cast image to float32\n",
        "        image /= 255.0\n",
        "        image = tf.keras.applications.efficientnet.preprocess_input(image)  # Preprocess image for EfficientNet\n",
        "        features = base_model(tf.expand_dims(image, 0))  # Compute EfficientNet features\n",
        "        features = tf.squeeze(GlobalAveragePooling2D()(features))\n",
        "        all_tensors_1=all_tensors_1.write(i, features)\n",
        "    stacked_tensor_1 = all_tensors_1.stack()\n",
        "    return stacked_tensor_1\n",
        "\n",
        "def preprocess_and_compute_features_seg(a):\n",
        "    # a,b,c,d,e,f,g =x\n",
        "\n",
        "    base_model = EfficientNetB0(include_top=False, weights='imagenet')\n",
        "    all_tensors_2 = tf.TensorArray(dtype=tf.float32, size=len(a), dynamic_size=True, clear_after_read=False)\n",
        "\n",
        "\n",
        "    for i in range(len(a)):\n",
        "        flip_condition = tf.strings.regex_full_match(a[i], \".*flip.*\")\n",
        "\n",
        "        image = tf.io.read_file(a[i])  # Read image file\n",
        "        image = tf.image.decode_png(image, channels=3)  # Decode PNG image\n",
        "        image = tf.cond(flip_condition, lambda: tf.image.flip_left_right(image), lambda: image)\n",
        "        image = tf.image.resize(image, (224, 224))  # Resize image to fit EfficientNet input size\n",
        "        image = tf.cast(image, tf.float32)  # Cast image to float32\n",
        "        image = tf.keras.applications.efficientnet.preprocess_input(image)  # Preprocess image for EfficientNet\n",
        "        features = base_model(tf.expand_dims(image, 0))  # Compute EfficientNet features\n",
        "        features = tf.squeeze(GlobalAveragePooling2D()(features))\n",
        "        all_tensors_2=all_tensors_2.write(i, features)\n",
        "    stacked_tensor_2 = all_tensors_2.stack()\n",
        "    return stacked_tensor_2\n",
        "def preprocess_bbox(g):\n",
        "    img_height=1080\n",
        "    img_width=1920\n",
        "\n",
        "    all_tensors_3 = tf.TensorArray(dtype=tf.float64, size=len(g), dynamic_size=True, clear_after_read=False)\n",
        "\n",
        "    for k in range(len(g)):\n",
        "        bbox = g[k]\n",
        "        x_1, y_1, x_2, y_2 = tf.split(bbox, 4, axis=-1)\n",
        "\n",
        "        # Normalize bounding box coordinates\n",
        "        y_1 = y_1 / img_height\n",
        "        x_1 = x_1 / img_width\n",
        "        y_2 = y_2 / img_height\n",
        "        x_2 = x_2 / img_width\n",
        "\n",
        "        normalized_bbox = tf.concat([x_1, y_1, x_2, y_2], axis=-1)\n",
        "        all_tensors_3 = all_tensors_3.write(k, normalized_bbox)\n",
        "\n",
        "    stacked_tensor_3 = all_tensors_3.stack()\n",
        "    return stacked_tensor_3\n",
        "\n",
        "\n",
        "\n",
        "dataset=ds\n",
        "# Map preprocessing and feature computation function to the dataset for the first and fourth elements\n",
        "dataset = dataset.map(lambda a,d,g,y: (preprocess_and_compute_features_img(a,g),preprocess_and_compute_features_seg(d),g,y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "dataset = dataset.map(lambda a,d,g,y: (a,d, preprocess_bbox(g),preprocess_bbox(y)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "dataset = dataset.filter(lambda a,d,g,y: tf.shape(a[0])[0] > 0)\n",
        "def prepare_dataset(a,d,g,y):\n",
        "    x = (a,d,g)  # Combine inputs into\n",
        "    return x, y\n",
        "\n",
        "# Map the dataset to prepare the data\n",
        "dataset = dataset.map(prepare_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "# Prefetch the dataset\n",
        "dataset = dataset.prefetch(tf.data.AUTOTUNE).batch(8, num_parallel_calls=tf.data.AUTOTUNE).cache()\n",
        "# Filter out examples with empty images (assuming image is the first element in the dataset tuple)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6PDvl2Pjf1g",
        "outputId": "0b1c2d33-39f5-4c87-bff4-3dc5eba29108"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8, 15, 1280)\n"
          ]
        }
      ],
      "source": [
        "# Example usage of the dataset\n",
        "for x,y in dataset.take(1):\n",
        "   a,d,g=x\n",
        "print(a.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2a-axInVDN8"
      },
      "source": [
        "# Conditional Variational Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4p8rmqLTAdg"
      },
      "outputs": [],
      "source": [
        "hidden_dim = 16\n",
        "batch_size = 4\n",
        "num_classes = 10\n",
        "embedding_dim=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAm3TRoLTDuQ"
      },
      "outputs": [],
      "source": [
        "def dropout_and_batchnorm(x):\n",
        "    return Dropout(0.3)(BatchNormalization()(x))\n",
        "\n",
        "def noiser(args):\n",
        "    global mean, log_var\n",
        "    mean, log_var = args\n",
        "    N = K.random_normal(shape=(batch_size, hidden_dim), mean=0., stddev=1.0)\n",
        "    return K.exp(log_var / 2) * N + mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnSWlRv05uTu",
        "outputId": "d9bef7d1-a936-4d15-85eb-5318c12527fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_13 (InputLayer)          [(None, 15, 4)]      0           []                               \n",
            "                                                                                                  \n",
            " bidirectional_6 (Bidirectional  (None, 15, 64)      9472        ['input_13[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " bidirectional_7 (Bidirectional  (None, 32)          10368       ['bidirectional_6[0][0]']        \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " input_14 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " concatenate_6 (Concatenate)    (None, 42)           0           ['bidirectional_7[0][0]',        \n",
            "                                                                  'input_14[0][0]']               \n",
            "                                                                                                  \n",
            " dense_30 (Dense)               (None, 256)          11008       ['concatenate_6[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 256)         1024        ['dense_30[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)           (None, 256)          0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " dense_31 (Dense)               (None, 128)          32896       ['dropout_12[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 128)         512         ['dense_31[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)           (None, 128)          0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " dense_32 (Dense)               (None, 64)           8256        ['dropout_13[0][0]']             \n",
            "                                                                                                  \n",
            " dense_33 (Dense)               (None, 64)           8256        ['dropout_13[0][0]']             \n",
            "                                                                                                  \n",
            " latent_space (Lambda)          (8, 64)              0           ['dense_32[0][0]',               \n",
            "                                                                  'dense_33[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 81,792\n",
            "Trainable params: 81,024\n",
            "Non-trainable params: 768\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "\n",
        "lstm_units = 32  # Adjust based on your data complexity\n",
        "embedding_dim = 10  # Adjust based on your embedding size\n",
        "hidden_dim = 64  # Adjust based on desired latent space size\n",
        "\n",
        "# Input layers for image data and embedding vector\n",
        "model_input = Input(shape=(15, 4))\n",
        "lb = Input(shape=(embedding_dim,))\n",
        "\n",
        "# Process image data with Bidirectional LSTMs\n",
        "encoded_bbox = Bidirectional(LSTM(lstm_units, return_sequences=True))(model_input)\n",
        "encoded_bbox = Bidirectional(LSTM(lstm_units // 2, return_sequences=False))(encoded_bbox)  # Reduce units in later layers\n",
        "\n",
        "# Concatenate encoded image and embedding (assuming encoded_image has shape (batch_size, 45, lstm_units * 2))\n",
        "x = Concatenate(axis=-1)([encoded_bbox, lb])  # Explicitly specify concatenation axis\n",
        "\n",
        "# Dense layers for feature extraction\n",
        "x = Dense(256, activation=\"relu\")(x)  # Adjust units if needed\n",
        "x = dropout_and_batchnorm(x)\n",
        "x = Dense(128, activation=\"relu\")(x)  # Adjust units if needed\n",
        "x = dropout_and_batchnorm(x)\n",
        "\n",
        "# Latent space representation\n",
        "mean = Dense(hidden_dim)(x)\n",
        "log_var = Dense(hidden_dim)(x)\n",
        "h = Lambda(noiser, output_shape=(hidden_dim,), name=\"latent_space\")([mean, log_var])\n",
        "\n",
        "# Encoder model\n",
        "encoder = keras.Model([model_input, lb], h, name=\"encoder\")\n",
        "encoder.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieBR--FiHh-v",
        "outputId": "b4623a35-951e-4c79-aee2-581fa78652c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"decoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_15 (InputLayer)          [(None, 64)]         0           []                               \n",
            "                                                                                                  \n",
            " input_16 (InputLayer)          [(None, 10)]         0           []                               \n",
            "                                                                                                  \n",
            " concatenate_7 (Concatenate)    (None, 74)           0           ['input_15[0][0]',               \n",
            "                                                                  'input_16[0][0]']               \n",
            "                                                                                                  \n",
            " dense_34 (Dense)               (None, 128)          9600        ['concatenate_7[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 128)         512         ['dense_34[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)           (None, 128)          0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " dense_35 (Dense)               (None, 256)          33024       ['dropout_14[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 256)         1024        ['dense_35[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)           (None, 256)          0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " repeat_vector_3 (RepeatVector)  (None, 45, 256)     0           ['dropout_15[0][0]']             \n",
            "                                                                                                  \n",
            " lstm_14 (LSTM)                 (None, 45, 16)       17472       ['repeat_vector_3[0][0]']        \n",
            "                                                                                                  \n",
            " lstm_15 (LSTM)                 (None, 45, 32)       6272        ['lstm_14[0][0]']                \n",
            "                                                                                                  \n",
            " time_distributed_3 (TimeDistri  (None, 45, 4)       132         ['lstm_15[0][0]']                \n",
            " buted)                                                                                           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 68,036\n",
            "Trainable params: 67,268\n",
            "Non-trainable params: 768\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Decoder Inputs\n",
        "decoder_input = Input(shape=(hidden_dim,))  # Input for latent representation\n",
        "lb_dec = Input(shape=(embedding_dim,))     # Input for label embedding\n",
        "\n",
        "# Concatenate latent representation and label embedding\n",
        "x = Concatenate(axis=-1)([decoder_input, lb_dec])\n",
        "\n",
        "# Dense layers for feature reconstruction (adjust units as needed)\n",
        "x = Dense(128, activation=\"relu\")(x)\n",
        "x = dropout_and_batchnorm(x)\n",
        "x = Dense(256, activation=\"relu\")(x)\n",
        "x = dropout_and_batchnorm(x)\n",
        "\n",
        "# Replicate features for each time step\n",
        "x = RepeatVector(45)(x)  # Number of time steps in the output sequence\n",
        "\n",
        "# Generate sequence using LSTM layers\n",
        "x = LSTM(lstm_units // 2, return_sequences=True)(x)\n",
        "x = LSTM(lstm_units, return_sequences=True)(x)\n",
        "\n",
        "# Output layer for image data reconstruction\n",
        "decoded_image = TimeDistributed(Dense(4, activation=\"linear\"))(x)  # Adjust for your output\n",
        "\n",
        "# Decoder model\n",
        "decoder = tf.keras.Model([decoder_input, lb_dec], decoded_image, name=\"decoder\")\n",
        "decoder.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Bz77ttKTLND"
      },
      "outputs": [],
      "source": [
        "def vae_loss(x, y):\n",
        "    x = K.reshape(x, shape=(batch_size, 45*4))\n",
        "    y = K.reshape(y, shape=(batch_size, 45*4))\n",
        "    loss = K.sum(K.square(x-y), axis=-1)\n",
        "    kl_loss = -0.5 * K.sum(1 + log_var - K.square(mean) - K.exp(log_var), axis=-1)\n",
        "    return loss + kl_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87GcOtWUFKw1"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "class TubeletEmbedding(layers.Layer):\n",
        "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim =embed_dim\n",
        "        self.patch_size = patch_size\n",
        "        self.projection =tf.keras.layers.Conv1D(\n",
        "    self.embed_dim,\n",
        "    3,\n",
        "    strides=1,\n",
        "    padding='same',\n",
        "    data_format=None,\n",
        "    dilation_rate=1,\n",
        "    groups=1,\n",
        "    activation=None,\n",
        "    use_bias=True,\n",
        "    kernel_initializer='glorot_uniform',\n",
        "    bias_initializer='zeros',\n",
        "    kernel_regularizer=None,\n",
        "    bias_regularizer=None,\n",
        "    activity_regularizer=None,\n",
        "    kernel_constraint=None,\n",
        "    bias_constraint=None,\n",
        "    **kwargs\n",
        ")\n",
        "        self.projection2 = layers.GRU(self.embed_dim, return_sequences=True, return_state=True)\n",
        "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
        "\n",
        "    def call(self, videos, vid):\n",
        "        if vid==0:\n",
        "          projected_patches = self.projection(videos)\n",
        "\n",
        "        elif vid==1:\n",
        "          projected_patches,_ = self.projection2(videos)\n",
        "\n",
        "\n",
        "        flattened_patches = self.flatten(projected_patches)\n",
        "        return flattened_patches\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"patch_size\": self.patch_size,\n",
        "        })\n",
        "        return config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2qpXvKSFOqI"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PositionalEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        _, num_tokens, _ = input_shape\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_tokens, output_dim=self.embed_dim\n",
        "        )\n",
        "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
        "\n",
        "    def call(self, encoded_tokens):\n",
        "        # Encode the positions and add it to the encoded tokens\n",
        "        encoded_positions = self.position_embedding(self.positions)\n",
        "        encoded_tokens = encoded_tokens + encoded_positions\n",
        "        return encoded_tokens\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim\n",
        "\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56Z1szcNFStO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class DenoisingUNet(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, **kwargs):\n",
        "        super(DenoisingUNet, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.encoder1 = self.conv_block(embed_dim // 4)\n",
        "        self.encoder2 = self.conv_block(embed_dim // 2)\n",
        "        self.middle = self.conv_block(embed_dim)\n",
        "        self.decoder2 = self.deconv_block(embed_dim // 2)\n",
        "        self.decoder1 = self.deconv_block(embed_dim // 4)\n",
        "        self.final_conv = layers.Conv1D(embed_dim, kernel_size=1, padding='same')\n",
        "\n",
        "    def conv_block(self, filters):\n",
        "        block = tf.keras.Sequential([\n",
        "            layers.Conv1D(filters, kernel_size=3, padding='same', activation='relu'),\n",
        "            layers.Conv1D(filters, kernel_size=3, padding='same', activation='relu')\n",
        "        ])\n",
        "        return block\n",
        "\n",
        "    def deconv_block(self, filters):\n",
        "        block = tf.keras.Sequential([\n",
        "            layers.Conv1DTranspose(filters, kernel_size=3, strides=2, padding='same', activation='relu'),\n",
        "            layers.Conv1D(filters, kernel_size=3, padding='same', activation='relu')\n",
        "        ])\n",
        "        return block\n",
        "\n",
        "    def pad_or_crop(self, tensor, target_shape):\n",
        "        current_shape = tf.shape(tensor)[1]\n",
        "        diff = target_shape - current_shape\n",
        "        pad_left = tf.maximum(diff // 2, 0)\n",
        "        pad_right = tf.maximum(diff - pad_left, 0)\n",
        "        crop_left = tf.maximum(-diff // 2, 0)\n",
        "        crop_right = tf.maximum(-diff - crop_left, 0)\n",
        "        tensor = tensor[:, crop_left:current_shape-crop_right, :]\n",
        "        tensor = tf.pad(tensor, [[0, 0], [pad_left, pad_right], [0, 0]], mode='CONSTANT')\n",
        "        return tensor\n",
        "\n",
        "    def call(self, x):\n",
        "        e1 = self.encoder1(x)\n",
        "        e2 = self.encoder2(layers.MaxPooling1D(pool_size=2)(e1))\n",
        "        m = self.middle(layers.MaxPooling1D(pool_size=2)(e2))\n",
        "        d2 = self.decoder2(layers.UpSampling1D(size=2)(m))\n",
        "        d2 = self.pad_or_crop(d2, tf.shape(e2)[1])\n",
        "        d2 = layers.Concatenate()([d2, e2])\n",
        "        d1 = self.decoder1(layers.UpSampling1D(size=2)(d2))\n",
        "        d1 = self.pad_or_crop(d1, tf.shape(e1)[1])\n",
        "        d1 = layers.Concatenate()([d1, e1])\n",
        "        return self.final_conv(d1)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim\n",
        "        })\n",
        "        return config\n",
        "class PDAM(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim=128, max_iters=5, tolerance=1e-3, noise_std=0.1, **kwargs):\n",
        "        super(PDAM, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_iters = max_iters\n",
        "        self.tolerance = tolerance\n",
        "        self.noise_std = noise_std\n",
        "        self.query = layers.Dense(units=embed_dim)\n",
        "        self.key = layers.Dense(units=embed_dim)\n",
        "        self.value = layers.Dense(units=embed_dim)\n",
        "        self.denoise_net = DenoisingUNet(embed_dim)\n",
        "\n",
        "    def add_noise(self, tensor):\n",
        "        noise = tf.random.normal(shape=tf.shape(tensor), mean=0.0, stddev=self.noise_std, dtype=tf.float32)\n",
        "        return tensor + noise\n",
        "\n",
        "    def self_attention(self, Q, K, V):\n",
        "        d_k = tf.cast(self.embed_dim, dtype=tf.float32)\n",
        "        score = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(d_k)\n",
        "        score = tf.nn.softmax(score, axis=-1)\n",
        "        Z = tf.matmul(score, V)\n",
        "        return Z\n",
        "\n",
        "    def call(self, x1, x2):\n",
        "        Q = self.query(x1)\n",
        "        K = self.key(x1)\n",
        "        V = self.value(x2)\n",
        "        Q = self.add_noise(Q)\n",
        "        K = self.add_noise(K)\n",
        "        V = self.add_noise(V)\n",
        "        Z = self.self_attention(Q, K, V)\n",
        "        prev_Z = tf.zeros_like(Z)\n",
        "\n",
        "        def loop_cond(i, Z, prev_Z, Q, K, V):\n",
        "            return tf.logical_and(tf.less(i, self.max_iters), tf.greater(tf.reduce_mean(tf.abs(Z - prev_Z)), self.tolerance))\n",
        "\n",
        "        def loop_body(i, Z, prev_Z, Q, K, V):\n",
        "            prev_Z = Z\n",
        "            Z = self.self_attention(Q, K, V)\n",
        "            Z = tf.nn.softmax(self.denoise_net(Z), axis=-1)\n",
        "            Q, K, V = Z, Z, Z\n",
        "            Q = self.add_noise(Q)\n",
        "            K = self.add_noise(K)\n",
        "            V = self.add_noise(V)\n",
        "            return tf.add(i, 1), Z, prev_Z, Q, K, V\n",
        "\n",
        "        i = tf.constant(0)\n",
        "        i, Z, prev_Z, Q, K, V = tf.while_loop(loop_cond, loop_body, [i, Z, prev_Z, Q, K, V], shape_invariants=[i.get_shape(), tf.TensorShape([None, None, self.embed_dim]), tf.TensorShape([None, None, self.embed_dim]), Q.get_shape(), K.get_shape(), V.get_shape()])\n",
        "        return Z\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"max_iters\": self.max_iters,\n",
        "            \"tolerance\": self.tolerance,\n",
        "            \"noise_std\": self.noise_std\n",
        "        })\n",
        "        return config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5qYLLM_fuBl",
        "outputId": "89a1ad2d-3431-41af-ba08-48a219961c54"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ],
      "source": [
        "intent_model = load_model('/content/drive/MyDrive/obj-5/intent_model.h5', custom_objects={\n",
        "    'TubeletEmbedding': TubeletEmbedding,\n",
        "    'PositionalEncoder': PositionalEncoder,\n",
        "    'PDAM': PDAM,\n",
        "    'DenoisingUNet': DenoisingUNet\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mVM4M5XI00O",
        "outputId": "9d5f01b3-fbc6-4755-e25a-bf38f03fe01b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_12\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)           [(None, 15, 1280)]   0           []                               \n",
            "                                                                                                  \n",
            " input_9 (InputLayer)           [(None, 15, 4)]      0           []                               \n",
            "                                                                                                  \n",
            " input_8 (InputLayer)           [(None, 15, 1280)]   0           []                               \n",
            "                                                                                                  \n",
            " tubelet_embedding_2 (TubeletEm  (None, 15, 128)     543104      ['input_7[0][0]',                \n",
            " bedding)                                                         'input_9[0][0]',                \n",
            "                                                                  'input_8[0][0]']                \n",
            "                                                                                                  \n",
            " positional_encoder_2 (Position  (None, 15, 128)     1920        ['tubelet_embedding_2[0][0]',    \n",
            " alEncoder)                                                       'tubelet_embedding_2[2][0]']    \n",
            "                                                                                                  \n",
            " layer_normalization_16 (LayerN  (None, 15, 128)     256         ['positional_encoder_2[0][0]']   \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_17 (LayerN  (None, 15, 128)     256         ['tubelet_embedding_2[1][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_20 (LayerN  (None, 15, 128)     256         ['positional_encoder_2[1][0]']   \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_21 (LayerN  (None, 15, 128)     256         ['tubelet_embedding_2[1][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " pdam_4 (PDAM)                  (None, None, 128)    218240      ['layer_normalization_16[0][0]', \n",
            "                                                                  'layer_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " pdam_5 (PDAM)                  (None, None, 128)    218240      ['layer_normalization_20[0][0]', \n",
            "                                                                  'layer_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 15, 128)      0           ['pdam_4[0][0]',                 \n",
            "                                                                  'positional_encoder_2[0][0]']   \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 15, 128)      0           ['pdam_5[0][0]',                 \n",
            "                                                                  'positional_encoder_2[1][0]']   \n",
            "                                                                                                  \n",
            " sequential_22 (Sequential)     (None, 15, 128)      131712      ['add_4[0][0]',                  \n",
            "                                                                  'add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " encoded_R (Add)                (None, 15, 128)      0           ['sequential_22[0][0]',          \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " encoded_S (Add)                (None, 15, 128)      0           ['sequential_22[1][0]',          \n",
            "                                                                  'add_5[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,114,240\n",
            "Trainable params: 1,114,240\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "intent_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrWNx2sfp4ch",
        "outputId": "de179259-3c4b-4d4a-b756-a452600b89d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 15, 1024)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class CustomFeatureFusionModule(tf.keras.layers.Layer):\n",
        "    def __init__(self, timesteps=15, attention_dim=256, **kwargs):\n",
        "        super(CustomFeatureFusionModule, self).__init__(**kwargs)\n",
        "        self.timesteps = timesteps\n",
        "        self.attention_dim = attention_dim\n",
        "        self.dense1 = layers.Dense(units=4 * attention_dim, activation=tf.nn.relu)\n",
        "        self.dense2 = layers.Dense(units=4 * attention_dim, activation=tf.nn.gelu)\n",
        "        self.dense3 = layers.Dense(units=4 * attention_dim)\n",
        "        self.attention = layers.Attention()\n",
        "        self.multiply = layers.Multiply()\n",
        "        self.add = layers.Add()\n",
        "\n",
        "    def call(self, rgb_features, seg_features):\n",
        "        # Concatenate features\n",
        "        gated_features = tf.concat([rgb_features, seg_features], axis=-1)  # (batch_size, timesteps, 2 * attention_dim)\n",
        "        gated_features = self.dense1(gated_features)\n",
        "\n",
        "        # First branch with dense layer and GELU activation\n",
        "        x1 = self.dense2(gated_features)\n",
        "\n",
        "        # Second branch with dense layer (no activation)\n",
        "        x2 = self.dense3(gated_features)\n",
        "\n",
        "        # Attention scores (scaled dot-product)\n",
        "        attention_weights = self.attention([x1, x2])\n",
        "        attentive_features = self.multiply([attention_weights, x2])\n",
        "        final_attr = self.add([attentive_features, gated_features])\n",
        "\n",
        "        return final_attr\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'timesteps': self.timesteps,\n",
        "            'attention_dim': self.attention_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "# Example usage\n",
        "timesteps = 15\n",
        "attention_dim = 256\n",
        "rgb_features = tf.random.normal((32, timesteps, 128))  # Example input\n",
        "seg_features = tf.random.normal((32, timesteps, 128))  # Example input\n",
        "\n",
        "cffm_layer = CustomFeatureFusionModule(timesteps=timesteps, attention_dim=attention_dim)\n",
        "output = cffm_layer(rgb_features, seg_features)\n",
        "print(output.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkrHJCuXePHI",
        "outputId": "df261170-7feb-4e51-a65d-125f945978f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"cvae_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder (Functional)        (8, 64)                   81792     \n",
            "                                                                 \n",
            " decoder (Functional)        (None, 45, 4)             68036     \n",
            "                                                                 \n",
            " dense_52 (Dense)            multiple                  153610    \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         multiple                  0         \n",
            "                                                                 \n",
            " model_12 (Functional)       [(None, 15, 128),         1114240   \n",
            "                              (None, 15, 128)]                   \n",
            "                                                                 \n",
            " custom_feature_fusion_modul  multiple                 2362368   \n",
            " e_2 (CustomFeatureFusionMod                                     \n",
            " ule)                                                            \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,780,046\n",
            "Trainable params: 3,778,510\n",
            "Non-trainable params: 1,536\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "class CVAE(tf.keras.Model):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super(CVAE, self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.cond_emb = Dense(embedding_dim)\n",
        "    self.flatten= Flatten()\n",
        "    self.intent= intent_model\n",
        "    self.cffm_layer = CustomFeatureFusionModule(timesteps=timesteps, attention_dim=attention_dim)\n",
        "\n",
        "\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # Extract data and label information\n",
        "    img, seg, past = inputs\n",
        "\n",
        "    rep1, rep2= self.intent([img, seg, past], training=True)\n",
        "    context= self.cffm_layer(rep1, rep2)\n",
        "    emb= self.flatten(context)\n",
        "\n",
        "    z = self.encoder([past, self.cond_emb(emb)])\n",
        "    # Decode the latent representation and label\n",
        "    reconstructed = self.decoder([z, self.cond_emb(emb)])\n",
        "    return reconstructed\n",
        "\n",
        "# Create the CVAE model\n",
        "cvae = CVAE(encoder, decoder)\n",
        "batch_size=8\n",
        "# Create a sample input (example image and label)\n",
        "sample_img = tf.random.normal(shape=(batch_size, 15,1280))\n",
        "\n",
        "sample_seg = tf.random.uniform(shape=(batch_size,15,1280))\n",
        "\n",
        "sample_past = tf.random.uniform(shape=(batch_size,15,4))\n",
        "\n",
        "cvae(inputs=[sample_img, sample_seg, sample_past])  # Call the model with sample data\n",
        "cvae.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "04kocg7ne2ql",
        "outputId": "869d8a51-76e8-418d-9561-344b66db9e7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "    108/Unknown - 3809s 35s/step - loss: 75.8103"
          ]
        }
      ],
      "source": [
        "cvae.compile(optimizer=\"adam\", loss=vae_loss)\n",
        "cvae.fit(dataset, epochs=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMHYEsnkhs4e"
      },
      "outputs": [],
      "source": [
        "h = cvae.predict(testgen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LiW6al6snIY"
      },
      "outputs": [],
      "source": [
        "def get_latent_representation(model, data_future, data_past):\n",
        "  \"\"\"\n",
        "  Extracts the latent representation from the encoder.\n",
        "\n",
        "  Args:\n",
        "      model: The CVAE model.\n",
        "      data_future: The future data (shape: (batch_size, 45, 4)).\n",
        "      data_past: The past data (shape: (batch_size, 15, 4)).\n",
        "\n",
        "  Returns:\n",
        "      A tensor representing the latent representation (shape: (batch_size, latent_dim)).\n",
        "  \"\"\"\n",
        "  # Extract latent representation\n",
        "  z = model.encoder([data_future, model.cond_emb(model.flatten(data_past))])\n",
        "  return z\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5j-zC296odS"
      },
      "outputs": [],
      "source": [
        "len(testgen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVchmXPyA-Rm"
      },
      "outputs": [],
      "source": [
        "x, y = testgen[0][0]\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zqlt2KAHuieW"
      },
      "outputs": [],
      "source": [
        "# all_latent_vectors = []\n",
        "all_latent_vectors = tf.zeros(shape=(0, 16))  # Replace 16 with your actual latent dimension\n",
        "\n",
        "for i in range(len(testgen)):\n",
        "  # Access future and past data from the current batch\n",
        "  future_data = testgen[i][0][0]\n",
        "  past_data = testgen[i][0][1]\n",
        "\n",
        "  # Extract latent representation\n",
        "  latent_representation = get_latent_representation(cvae, future_data, past_data)\n",
        "\n",
        "  # Append the latent representation to the list\n",
        "  all_latent_vectors = tf.concat([all_latent_vectors, latent_representation], axis=0)\n",
        "\n",
        "# Now you have all the latent vectors in the 'all_latent_vectors' list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0EFiyvb1JKl"
      },
      "outputs": [],
      "source": [
        "all_latent_vectors=np.array(all_latent_vectors)\n",
        "all_latent_vectors.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXIj3QSXypHJ"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# Reduce latent representation to 2D for visualization (optional)\n",
        "if hidden_dim > 2:\n",
        "  from sklearn.decomposition import PCA\n",
        "  pca = PCA(n_components=2)\n",
        "  latent_representation_pca = pca.fit_transform(all_latent_vectors)\n",
        "\n",
        "# Plot the latent space\n",
        "plt.scatter(latent_representation_pca[:,0], latent_representation_pca[:,1])  # Assuming 2D latent space\n",
        "plt.xlabel(\"Latent Dimension 1\")\n",
        "plt.ylabel(\"Latent Dimension 2\")\n",
        "plt.title(\"Latent Space Visualization\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "M9laZQWlrmg-",
        "bATUJdV9rmhC",
        "IIyq12PxAUOS"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}